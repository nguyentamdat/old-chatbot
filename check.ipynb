{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### insert database\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound2unicode(text):\n",
    "  #https://gist.github.com/redphx/9320735`\n",
    "  text = text.replace(\"\\u0065\\u0309\", \"\\u1EBB\")    # ẻ\n",
    "  text = text.replace(\"\\u0065\\u0301\", \"\\u00E9\")    # é\n",
    "  text = text.replace(\"\\u0065\\u0300\", \"\\u00E8\")    # è\n",
    "  text = text.replace(\"\\u0065\\u0323\", \"\\u1EB9\")    # ẹ\n",
    "  text = text.replace(\"\\u0065\\u0303\", \"\\u1EBD\")    # ẽ\n",
    "  text = text.replace(\"\\u00EA\\u0309\", \"\\u1EC3\")    # ể\n",
    "  text = text.replace(\"\\u00EA\\u0301\", \"\\u1EBF\")    # ế\n",
    "  text = text.replace(\"\\u00EA\\u0300\", \"\\u1EC1\")    # ề\n",
    "  text = text.replace(\"\\u00EA\\u0323\", \"\\u1EC7\")    # ệ\n",
    "  text = text.replace(\"\\u00EA\\u0303\", \"\\u1EC5\")    # ễ\n",
    "  text = text.replace(\"\\u0079\\u0309\", \"\\u1EF7\")    # ỷ\n",
    "  text = text.replace(\"\\u0079\\u0301\", \"\\u00FD\")    # ý\n",
    "  text = text.replace(\"\\u0079\\u0300\", \"\\u1EF3\")    # ỳ\n",
    "  text = text.replace(\"\\u0079\\u0323\", \"\\u1EF5\")    # ỵ\n",
    "  text = text.replace(\"\\u0079\\u0303\", \"\\u1EF9\")    # ỹ\n",
    "  text = text.replace(\"\\u0075\\u0309\", \"\\u1EE7\")    # ủ\n",
    "  text = text.replace(\"\\u0075\\u0301\", \"\\u00FA\")    # ú\n",
    "  text = text.replace(\"\\u0075\\u0300\", \"\\u00F9\")    # ù\n",
    "  text = text.replace(\"\\u0075\\u0323\", \"\\u1EE5\")    # ụ\n",
    "  text = text.replace(\"\\u0075\\u0303\", \"\\u0169\")    # ũ\n",
    "  text = text.replace(\"\\u01B0\\u0309\", \"\\u1EED\")    # ử\n",
    "  text = text.replace(\"\\u01B0\\u0301\", \"\\u1EE9\")    # ứ\n",
    "  text = text.replace(\"\\u01B0\\u0300\", \"\\u1EEB\")    # ừ\n",
    "  text = text.replace(\"\\u01B0\\u0323\", \"\\u1EF1\")    # ự\n",
    "  text = text.replace(\"\\u01B0\\u0303\", \"\\u1EEF\")    # ữ\n",
    "  text = text.replace(\"\\u0069\\u0309\", \"\\u1EC9\")    # ỉ\n",
    "  text = text.replace(\"\\u0069\\u0301\", \"\\u00ED\")    # í\n",
    "  text = text.replace(\"\\u0069\\u0300\", \"\\u00EC\")    # ì\n",
    "  text = text.replace(\"\\u0069\\u0323\", \"\\u1ECB\")    # ị\n",
    "  text = text.replace(\"\\u0069\\u0303\", \"\\u0129\")    # ĩ\n",
    "  text = text.replace(\"\\u006F\\u0309\", \"\\u1ECF\")    # ỏ\n",
    "  text = text.replace(\"\\u006F\\u0301\", \"\\u00F3\")    # ó\n",
    "  text = text.replace(\"\\u006F\\u0300\", \"\\u00F2\")    # ò\n",
    "  text = text.replace(\"\\u006F\\u0323\", \"\\u1ECD\")    # ọ\n",
    "  text = text.replace(\"\\u006F\\u0303\", \"\\u00F5\")    # õ\n",
    "  text = text.replace(\"\\u01A1\\u0309\", \"\\u1EDF\")    # ở\n",
    "  text = text.replace(\"\\u01A1\\u0301\", \"\\u1EDB\")    # ớ\n",
    "  text = text.replace(\"\\u01A1\\u0300\", \"\\u1EDD\")    # ờ\n",
    "  text = text.replace(\"\\u01A1\\u0323\", \"\\u1EE3\")    # ợ\n",
    "  text = text.replace(\"\\u01A1\\u0303\", \"\\u1EE1\")    # ỡ\n",
    "  text = text.replace(\"\\u00F4\\u0309\", \"\\u1ED5\")    # ổ\n",
    "  text = text.replace(\"\\u00F4\\u0301\", \"\\u1ED1\")    # ố\n",
    "  text = text.replace(\"\\u00F4\\u0300\", \"\\u1ED3\")    # ồ\n",
    "  text = text.replace(\"\\u00F4\\u0323\", \"\\u1ED9\")    # ộ\n",
    "  text = text.replace(\"\\u00F4\\u0303\", \"\\u1ED7\")    # ỗ\n",
    "  text = text.replace(\"\\u0061\\u0309\", \"\\u1EA3\")    # ả\n",
    "  text = text.replace(\"\\u0061\\u0301\", \"\\u00E1\")    # á\n",
    "  text = text.replace(\"\\u0061\\u0300\", \"\\u00E0\")    # à\n",
    "  text = text.replace(\"\\u0061\\u0323\", \"\\u1EA1\")    # ạ\n",
    "  text = text.replace(\"\\u0061\\u0303\", \"\\u00E3\")    # ã\n",
    "  text = text.replace(\"\\u0103\\u0309\", \"\\u1EB3\")    # ẳ\n",
    "  text = text.replace(\"\\u0103\\u0301\", \"\\u1EAF\")    # ắ\n",
    "  text = text.replace(\"\\u0103\\u0300\", \"\\u1EB1\")    # ằ\n",
    "  text = text.replace(\"\\u0103\\u0323\", \"\\u1EB7\")    # ặ\n",
    "  text = text.replace(\"\\u0103\\u0303\", \"\\u1EB5\")    # ẵ\n",
    "  text = text.replace(\"\\u00E2\\u0309\", \"\\u1EA9\")    # ẩ\n",
    "  text = text.replace(\"\\u00E2\\u0301\", \"\\u1EA5\")    # ấ\n",
    "  text = text.replace(\"\\u00E2\\u0300\", \"\\u1EA7\")    # ầ\n",
    "  text = text.replace(\"\\u00E2\\u0323\", \"\\u1EAD\")    # ậ\n",
    "  text = text.replace(\"\\u00E2\\u0303\", \"\\u1EAB\")    # ẫ\n",
    "  text = text.replace(\"\\u0045\\u0309\", \"\\u1EBA\")    # Ẻ\n",
    "  text = text.replace(\"\\u0045\\u0301\", \"\\u00C9\")    # É\n",
    "  text = text.replace(\"\\u0045\\u0300\", \"\\u00C8\")    # È\n",
    "  text = text.replace(\"\\u0045\\u0323\", \"\\u1EB8\")    # Ẹ\n",
    "  text = text.replace(\"\\u0045\\u0303\", \"\\u1EBC\")    # Ẽ\n",
    "  text = text.replace(\"\\u00CA\\u0309\", \"\\u1EC2\")    # Ể\n",
    "  text = text.replace(\"\\u00CA\\u0301\", \"\\u1EBE\")    # Ế\n",
    "  text = text.replace(\"\\u00CA\\u0300\", \"\\u1EC0\")    # Ề\n",
    "  text = text.replace(\"\\u00CA\\u0323\", \"\\u1EC6\")    # Ệ\n",
    "  text = text.replace(\"\\u00CA\\u0303\", \"\\u1EC4\")    # Ễ\n",
    "  text = text.replace(\"\\u0059\\u0309\", \"\\u1EF6\")    # Ỷ\n",
    "  text = text.replace(\"\\u0059\\u0301\", \"\\u00DD\")    # Ý\n",
    "  text = text.replace(\"\\u0059\\u0300\", \"\\u1EF2\")    # Ỳ\n",
    "  text = text.replace(\"\\u0059\\u0323\", \"\\u1EF4\")    # Ỵ\n",
    "  text = text.replace(\"\\u0059\\u0303\", \"\\u1EF8\")    # Ỹ\n",
    "  text = text.replace(\"\\u0055\\u0309\", \"\\u1EE6\")    # Ủ\n",
    "  text = text.replace(\"\\u0055\\u0301\", \"\\u00DA\")    # Ú\n",
    "  text = text.replace(\"\\u0055\\u0300\", \"\\u00D9\")    # Ù\n",
    "  text = text.replace(\"\\u0055\\u0323\", \"\\u1EE4\")    # Ụ\n",
    "  text = text.replace(\"\\u0055\\u0303\", \"\\u0168\")    # Ũ\n",
    "  text = text.replace(\"\\u01AF\\u0309\", \"\\u1EEC\")    # Ử\n",
    "  text = text.replace(\"\\u01AF\\u0301\", \"\\u1EE8\")    # Ứ\n",
    "  text = text.replace(\"\\u01AF\\u0300\", \"\\u1EEA\")    # Ừ\n",
    "  text = text.replace(\"\\u01AF\\u0323\", \"\\u1EF0\")    # Ự\n",
    "  text = text.replace(\"\\u01AF\\u0303\", \"\\u1EEE\")    # Ữ\n",
    "  text = text.replace(\"\\u0049\\u0309\", \"\\u1EC8\")    # Ỉ\n",
    "  text = text.replace(\"\\u0049\\u0301\", \"\\u00CD\")    # Í\n",
    "  text = text.replace(\"\\u0049\\u0300\", \"\\u00CC\")    # Ì\n",
    "  text = text.replace(\"\\u0049\\u0323\", \"\\u1ECA\")    # Ị\n",
    "  text = text.replace(\"\\u0049\\u0303\", \"\\u0128\")    # Ĩ\n",
    "  text = text.replace(\"\\u004F\\u0309\", \"\\u1ECE\")    # Ỏ\n",
    "  text = text.replace(\"\\u004F\\u0301\", \"\\u00D3\")    # Ó\n",
    "  text = text.replace(\"\\u004F\\u0300\", \"\\u00D2\")    # Ò\n",
    "  text = text.replace(\"\\u004F\\u0323\", \"\\u1ECC\")    # Ọ\n",
    "  text = text.replace(\"\\u004F\\u0303\", \"\\u00D5\")    # Õ\n",
    "  text = text.replace(\"\\u01A0\\u0309\", \"\\u1EDE\")    # Ở\n",
    "  text = text.replace(\"\\u01A0\\u0301\", \"\\u1EDA\")    # Ớ\n",
    "  text = text.replace(\"\\u01A0\\u0300\", \"\\u1EDC\")    # Ờ\n",
    "  text = text.replace(\"\\u01A0\\u0323\", \"\\u1EE2\")    # Ợ\n",
    "  text = text.replace(\"\\u01A0\\u0303\", \"\\u1EE0\")    # Ỡ\n",
    "  text = text.replace(\"\\u00D4\\u0309\", \"\\u1ED4\")    # Ổ\n",
    "  text = text.replace(\"\\u00D4\\u0301\", \"\\u1ED0\")    # Ố\n",
    "  text = text.replace(\"\\u00D4\\u0300\", \"\\u1ED2\")    # Ồ\n",
    "  text = text.replace(\"\\u00D4\\u0323\", \"\\u1ED8\")    # Ộ\n",
    "  text = text.replace(\"\\u00D4\\u0303\", \"\\u1ED6\")    # Ỗ\n",
    "  text = text.replace(\"\\u0041\\u0309\", \"\\u1EA2\")    # Ả\n",
    "  text = text.replace(\"\\u0041\\u0301\", \"\\u00C1\")    # Á\n",
    "  text = text.replace(\"\\u0041\\u0300\", \"\\u00C0\")    # À\n",
    "  text = text.replace(\"\\u0041\\u0323\", \"\\u1EA0\")    # Ạ\n",
    "  text = text.replace(\"\\u0041\\u0303\", \"\\u00C3\")    # Ã\n",
    "  text = text.replace(\"\\u0102\\u0309\", \"\\u1EB2\")    # Ẳ\n",
    "  text = text.replace(\"\\u0102\\u0301\", \"\\u1EAE\")    # Ắ\n",
    "  text = text.replace(\"\\u0102\\u0300\", \"\\u1EB0\")    # Ằ\n",
    "  text = text.replace(\"\\u0102\\u0323\", \"\\u1EB6\")    # Ặ\n",
    "  text = text.replace(\"\\u0102\\u0303\", \"\\u1EB4\")    # Ẵ\n",
    "  text = text.replace(\"\\u00C2\\u0309\", \"\\u1EA8\")    # Ẩ\n",
    "  text = text.replace(\"\\u00C2\\u0301\", \"\\u1EA4\")    # Ấ\n",
    "  text = text.replace(\"\\u00C2\\u0300\", \"\\u1EA6\")    # Ầ\n",
    "  text = text.replace(\"\\u00C2\\u0323\", \"\\u1EAC\")    # Ậ\n",
    "  text = text.replace(\"\\u00C2\\u0303\", \"\\u1EAA\")    # Ẫ\n",
    "  return text\n",
    "\n",
    "def preprocess_message(message):\n",
    "    message = message.lower()\n",
    "    message = message.replace(',', ' , ')\n",
    "    message = message.replace('.', ' . ')\n",
    "    message = message.replace('!', ' ! ')\n",
    "    message = message.replace('&', ' & ')\n",
    "    message = message.replace('?', ' ? ')\n",
    "    message = message.replace('-', ' - ')\n",
    "    message = message.replace('(', ' ( ')\n",
    "    message = message.replace(')', ' ) ')\n",
    "    message = compound2unicode(message)\n",
    "    list_token = message.split(' ')\n",
    "    while '' in list_token:\n",
    "        list_token.remove('')\n",
    "    message = ' '.join(list_token)\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "a = [1,1,1,1]\n",
    "for i in range(len(a)):\n",
    "    a[i] =2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://caochanhduong:bikhungha1@ds261626.mlab.com:61626/activity?retryWrites=false')\n",
    "db = client.activity\n",
    "activities = db.activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_data/real_db_494_final.json','r') as db_file:\n",
    "    list_data = json.load(db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for data in list_data:\n",
    "    data[\"time_works_place_address_mapping\"] = []\n",
    "    if \"time_work_place_mapping\" in data:\n",
    "        del data[\"time_work_place_mapping\"]\n",
    "    if \"_id\" in data:\n",
    "        del data[\"_id\"]\n",
    "    for key in (list(data.keys())):\n",
    "        if key not in [\"_id\",\"time_work_place_mapping\"]:\n",
    "            for j in range(len(data[key])):\n",
    "                data[key][j] = preprocess_message(data[key][j])\n",
    "    activities.insert_one(data)\n",
    "    if i%10 == 0:\n",
    "        print(i)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name_activity', 'type_activity', 'holder', 'time', 'name_place', 'address', 'reward', 'contact', 'register', 'works', 'joiner'])\n",
      "------------------------type activity\n",
      "tuyển hỗ trợ viên\n",
      "hỗ trợ chương trình\n",
      "hỗ trợ triễn lãm\n",
      "buổi hỗ trợ\n",
      "tuyển ctv hỗ trợ\n",
      "hỗ trợ\n",
      "hỗ trợ gây quỹ\n"
     ]
    }
   ],
   "source": [
    "with open('real_dict_2000_new_only_delete_question_noti_new_and_space_newest.json','r') as dict_file:\n",
    "    real_dict = json.load(dict_file)\n",
    "    print(real_dict.keys())\n",
    "    list_holder = real_dict['holder']\n",
    "    list_name_place = real_dict['name_place']\n",
    "    list_name_activity = real_dict['name_activity']\n",
    "    list_joiner = real_dict['joiner']\n",
    "    list_type_activity = real_dict['type_activity']\n",
    "    list_reward = real_dict['reward']\n",
    "    list_works = real_dict['works']\n",
    "    \n",
    "    res = []\n",
    "\n",
    "#     print(\"------------------------joiner\")\n",
    "#     for joiner in list_joiner:\n",
    "#         if compound2unicode(\"bạn\") in compound2unicode(joiner):\n",
    "#             print(joiner)\n",
    "#     print(\"------------------------works\")\n",
    "#     for works in list_works:\n",
    "#         if compound2unicode(\"hoạt động\") in compound2unicode(works):\n",
    "#             print(works)\n",
    "            \n",
    "#     print(\"------------------------reward\")\n",
    "#     for reward in list_reward:\n",
    "# #         if \"khoa học và công nghệ\" in reward:\n",
    "#         print(reward)\n",
    "            \n",
    "    print(\"------------------------type activity\")\n",
    "    for type_activity in list_type_activity:\n",
    "        if compound2unicode(\"hỗ trợ\") in compound2unicode(type_activity):\n",
    "            print(type_activity)\n",
    "            \n",
    "#     print(\"------------------------holder\")\n",
    "#     for holder in list_holder:\n",
    "#         if compound2unicode(\"bách khoa\") in compound2unicode(holder):\n",
    "#             print(holder)\n",
    "            \n",
    "#     print(\"------------------------name_place\")\n",
    "#     for name_place in list_name_place:\n",
    "#         if compound2unicode(\"bách khoa\") in compound2unicode(name_place):\n",
    "#             print(name_place)\n",
    "#     print(\"------------------------name_activity\")\n",
    "#     for name_activity in list_name_activity:\n",
    "#         if \"bách khoa\" in name_activity:\n",
    "#             print(name_activity)\n",
    "#     for joiner in list_joiner:\n",
    "#         print(joiner)\n",
    "#         if name_activity.find(\"chương trình\") == 0:\n",
    "#             res.append(name_activity.replace(\"chương trình \",\"\"))\n",
    "#         else:\n",
    "#             res.append(name_activity)\n",
    "# #     print(res)\n",
    "#     real_dict['name_activity'] = res\n",
    "#     with open('real_dict_2000_new_only_delete_question_noti_new_and_space_newest.json', 'w+') as new_dict_file:\n",
    "#         json.dump(real_dict,new_dict_file,ensure_ascii=False)\n",
    "# print(list_holder)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494\n"
     ]
    }
   ],
   "source": [
    "with open('temp_data/real_db_769.json','r') as db_file:\n",
    "    list_data = json.load(db_file)\n",
    "#     print(list_data)\n",
    "    res = list_data\n",
    "    for data in list_data:\n",
    "        check_not_match_holder = False\n",
    "#         if 'name_activity' not in list(data.keys()):\n",
    "#             break\n",
    "        for holder in data['holder']:\n",
    "            if holder not in list_holder:\n",
    "                check_not_match_holder = True\n",
    "                break\n",
    "        if check_not_match_holder == True:\n",
    "            res.remove(data)\n",
    "            \n",
    "    for data in list_data:\n",
    "        check_not_match_name_place = False\n",
    "#         if 'name_activity' not in list(data.keys()):\n",
    "#             break\n",
    "        for name_place in data['name_place']:\n",
    "            if name_place not in list_name_place:\n",
    "                check_not_match_name_place = True\n",
    "                break\n",
    "        if check_not_match_name_place == True:\n",
    "            res.remove(data)\n",
    "    \n",
    "    \n",
    "    print(len(res))\n",
    "    with open('temp_data/real_db_494.json','w+') as new_db_file:\n",
    "        json.dump(res,new_db_file,ensure_ascii=False)\n",
    "    \n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\d\\d\\d\n"
     ]
    }
   ],
   "source": [
    "with open('list_constants.json','r') as constants_file:\n",
    "    list_constants = json.load(constants_file)\n",
    "    list_pattern_time = list_constants['list_pattern_time']\n",
    "    for pattern in list_pattern_time:\n",
    "        if re.findall(pattern,'305') != []:\n",
    "            print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp_agent_action_gen import *\n",
    "from message_handler import *\n",
    "from agen_response_gen import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_entity(intent,input_sentence):\n",
    "    print(\"duongcc\")\n",
    "    normalized_input_sentence = compound2unicode(input_sentence)\n",
    "    normalized_input_sentence = delete_extra_word(normalized_input_sentence,list_extra_word)\n",
    "    \n",
    "    result_entity_dict={}\n",
    "    list_order_entity_name=map_intent_to_list_order_entity_name[intent]\n",
    "    print(normalized_input_sentence)\n",
    "    if 'time' in list_order_entity_name:\n",
    "        for pattern_time in list_pattern_time:\n",
    "            if re.findall(pattern_time,normalized_input_sentence)!=[]:\n",
    "                # print(\"pattern_time :{0}\".format(pattern_time))\n",
    "                if 'time' not in result_entity_dict:\n",
    "                    result_entity_dict['time'] = delete_last_space_list(re.findall(pattern_time,normalized_input_sentence))\n",
    "                else:\n",
    "                    result_entity_dict['time'].extend(delete_last_space_list(re.findall(pattern_time,normalized_input_sentence)))\n",
    "                \n",
    "                normalized_input_sentence = re.sub(pattern_time,' '.join(['✪']*(pattern_time.count(' ')+1)),normalized_input_sentence)\n",
    "        # if 'time' in result_entity_dict:\n",
    "        #     print(result_entity_dict['time'])\n",
    "    if 'reward' in list_order_entity_name:\n",
    "        for pattern_reward in list_pattern_reward:\n",
    "            if re.findall(pattern_reward,normalized_input_sentence)!=[]:\n",
    "                print(\"pattern_reward :{0}\".format(pattern_reward))\n",
    "                if 'reward' not in result_entity_dict:\n",
    "                    result_entity_dict['reward'] = delete_last_space_list(re.findall(pattern_reward,normalized_input_sentence))\n",
    "                else:\n",
    "                    result_entity_dict['reward'].extend(delete_last_space_list(re.findall(pattern_reward,normalized_input_sentence)))\n",
    "                \n",
    "                normalized_input_sentence = re.sub(pattern_reward,' '.join(['✪']*(pattern_reward.count(' ')+1)),normalized_input_sentence)\n",
    "        # if 'reward' in result_entity_dict:\n",
    "        #     print(result_entity_dict['reward'])\n",
    "    matching_threshold = 0.0\n",
    "    longest_common_length, end_common_index = None, None\n",
    "    \n",
    "    map_entity_name_to_threshold={}\n",
    "    for entity_name in list_order_entity_name:\n",
    "        if entity_name in ['time','address']:\n",
    "            map_entity_name_to_threshold[entity_name]=1\n",
    "        elif entity_name in ['name_activity','contact','joiner','holder','type_activity','name_place']:\n",
    "            map_entity_name_to_threshold[entity_name]=2\n",
    "        elif entity_name in ['works','reward']:\n",
    "            map_entity_name_to_threshold[entity_name]=2\n",
    "        elif entity_name in ['register']:\n",
    "            map_entity_name_to_threshold[entity_name]=3\n",
    "\n",
    "\n",
    "    ordered_real_dict = OrderedDict()\n",
    "    for entity_name in map_intent_to_list_order_entity_name[intent]:\n",
    "        ordered_real_dict[entity_name] = real_dict[entity_name]\n",
    "    for entity_name, list_entity in ordered_real_dict.items():\n",
    "        # print(entity_name)\n",
    "        list_entity = [entity.lower() for entity in list_entity]\n",
    "        # print(\"input sentence: {0}\".format(normalized_input_sentence))\n",
    "        if entity_name in [\"works\",\"register\",\"reward\"]:\n",
    "            matching_threshold = 0.15\n",
    "        elif entity_name == \"joiner\":\n",
    "            matching_threshold = 0.2\n",
    "        else:\n",
    "            matching_threshold = 0.5\n",
    "        print(\"000. sentence:{0}\".format(normalized_input_sentence))\n",
    "        catch_entity_threshold_loop = 0\n",
    "        while True:\n",
    "            if catch_entity_threshold_loop > 5:\n",
    "                break\n",
    "            list_dict_longest_common_entity = find_entity_longest_common(normalized_input_sentence,list_entity,entity_name)\n",
    "#             print(list_dict_longest_common_entity)\n",
    "                #     [{'longest_common_entity_index': 0,\n",
    "                #   'longest_common_length': 3,\n",
    "                #   'end_common_index': 9}]\n",
    "            \n",
    "\n",
    "            ##find the most match longest common match (calculate by length of token match in sentence \n",
    "                                                                #/ length of entity )\n",
    "                            ##{'greatest_match_entity_index':0,'longest_common_length':3,'end_common_index':9}\n",
    "            if list_dict_longest_common_entity == []:\n",
    "                break\n",
    "            if list_dict_longest_common_entity[0]['longest_common_length'] < map_entity_name_to_threshold[entity_name] :\n",
    "                break\n",
    "            \n",
    "            list_sentence_token = normalized_input_sentence.split(' ')\n",
    "#             print(\"list_sentence_token :{0}\".format(list_sentence_token))\n",
    "            greatest_entity_index=None\n",
    "            greatest_common_length = None\n",
    "            greatest_end_common_index = None\n",
    "            max_match_entity = 0.0\n",
    "#             print(\"common entity :{0}\".format(list_dict_longest_common_entity))\n",
    "            for dict_longest_common_entity in list_dict_longest_common_entity:\n",
    "#                 print(\"0. dict_longest_common_entity: {0}\".format(dict_longest_common_entity))\n",
    "\n",
    "#                     print(\"duong\")\n",
    "#                 print(\"0.1 entity: {0}\".format(list_entity[dict_longest_common_entity['longest_common_entity_index']]))\n",
    "                longest_common_entity_index = dict_longest_common_entity['longest_common_entity_index']\n",
    "                longest_common_length = dict_longest_common_entity['longest_common_length']\n",
    "                end_common_index = dict_longest_common_entity['end_common_index']\n",
    "                \n",
    "                list_sentence_token_match = list_sentence_token[end_common_index - longest_common_length+1:end_common_index+1]\n",
    "                if entity_name == \"type_activity\":\n",
    "                    if \"ban chỉ huy\" in normalized_input_sentence or \"ban tổ chức\" in normalized_input_sentence or \"bch\" in normalized_input_sentence or \"btc\" in normalized_input_sentence:\n",
    "                        continue\n",
    "                    #nếu chỉ là các câu inform 1 entity mà câu đó không phải là câu inform tên 1 hoạt động thì không cần xét\n",
    "                    if \"inform\" not in intent or \"name_activity\" in intent:\n",
    "                        list_name_activity = ordered_real_dict['name_activity']\n",
    "                        check_in_name = False\n",
    "                        for name_activity in list_name_activity:\n",
    "                            #nếu loại hoạt động nằm trọn trong bất kì 1 tên hoạt động \n",
    "                            # thì không lấy\n",
    "                            if  name_activity.find(' '.join(list_sentence_token_match)) > 0:\n",
    "                                check_in_name = True\n",
    "                                break\n",
    "                        if check_in_name == True:\n",
    "                            continue\n",
    "                \n",
    "                if entity_name == \"holder\":\n",
    "                    # nếu holder mà trước đó có từ chỉ nơi chốn : ở, tại => không là holder mà là  \n",
    "                    # name_place\n",
    "                    if end_common_index - longest_common_length >= 0:\n",
    "                        if list_sentence_token[end_common_index - longest_common_length] in [\"ở\",\"tại\",\"trước\",\"sau\",\"trong\"]:\n",
    "                            if 'name_place' in result_entity_dict:\n",
    "            #                     result_entity_dict[entity_name].append(list_entity[greatest_entity_index])\n",
    "                                result_entity_dict['name_place'].append(' '.join(list_sentence_token_match))\n",
    "                            else:\n",
    "            #                     result_entity_dict[entity_name] = [list_entity[greatest_entity_index]]\n",
    "                                result_entity_dict['name_place'] = [' '.join(list_sentence_token_match)]\n",
    "                            list_sentence_token[end_common_index - longest_common_length +1 :end_common_index +1] = [\"✪\"]*longest_common_length\n",
    "                            normalized_input_sentence = ' '.join(list_sentence_token)\n",
    "                            continue\n",
    "\n",
    "#                 print(\"2. list_sentence_token_match : {0}\".format(list_sentence_token_match))\n",
    "                list_temp_longest_entity_token = list_entity[longest_common_entity_index].split(' ')\n",
    "#                 print(\"3. list_temp_longest_entity_token : {0}\".format(list_temp_longest_entity_token))\n",
    "                if entity_name in [\"works\",\"register\",\"reward\"]:\n",
    "#                     print(\"list_temp_longest_entity_token :{0}\".format(list_temp_longest_entity_token))\n",
    "#                     print(\"list_sentence_token_match :{0}\".format(list_sentence_token_match))\n",
    "                    _,longest_common_length_entity,end_common_index_entity = lcs_length_ta(list_temp_longest_entity_token,list_sentence_token_match)\n",
    "                    list_entity_token_match = list_temp_longest_entity_token[end_common_index_entity - longest_common_length_entity +1 :end_common_index_entity +1]\n",
    "                    score = len(list_entity_token_match)/float(len(list_temp_longest_entity_token))\n",
    "#                     print(\"list_entity_token_match: {0}\".format(list_entity_token_match))\n",
    "#                     print(\"list_temp_longest_entity_token:{0}\".format(list_temp_longest_entity_token))\n",
    "#                     print(\"score :{0}\".format(score))\n",
    "                    \n",
    "                else:\n",
    "                    score = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))\n",
    "                if score > max_match_entity:\n",
    "#                     max_match_entity = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))\n",
    "                    max_match_entity = score\n",
    "                    greatest_entity_index = longest_common_entity_index\n",
    "                    greatest_common_length = longest_common_length\n",
    "                    greatest_end_common_index = end_common_index\n",
    "#             print(list_sentence_token)\n",
    "#             print(greatest_common_length)\n",
    "#             print(greatest_end_common_index)\n",
    "#             print(\"longest_common_length: {0}\".format(longest_common_length))\n",
    "#             print(\"end_common_index: {0}\".format(end_common_index))\n",
    "#             print(\"1. greatest_common_length : {0}\".format(greatest_common_length))\n",
    "            # print(max_match_entity)\n",
    "            # print(\"2. greatest entity : {0}\".format(list_entity[greatest_entity_index]))\n",
    "#             print(\"2.1 greatest_end_common_index: {0}\".format(greatest_end_common_index))\n",
    "#             print(\"3. sentence match: {0}\".format(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1]))\n",
    "            if greatest_common_length != None:\n",
    "                if greatest_common_length >= map_entity_name_to_threshold[entity_name] and max_match_entity > matching_threshold:\n",
    "                    # if entity_name in ['name_activity','type_activity']:\n",
    "                    #     result = list_entity[greatest_entity_index]\n",
    "                    # else:\n",
    "                    #     result = ' '.join(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1])\n",
    "                    \n",
    "                    result = ' '.join(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1])\n",
    "                    if entity_name in result_entity_dict:\n",
    "    #                     result_entity_dict[entity_name].append(list_entity[greatest_entity_index])\n",
    "                        result_entity_dict[entity_name].append(result)\n",
    "                    else:\n",
    "    #                     result_entity_dict[entity_name] = [list_entity[greatest_entity_index]]\n",
    "                        result_entity_dict[entity_name] = [result]\n",
    "    #                 list_sentence_token = list_sentence_token[:greatest_end_common_index - greatest_common_length + 1] + list_sentence_token[greatest_end_common_index +1 :]\n",
    "                    list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1] = [\"✪\"]*greatest_common_length\n",
    "                    normalized_input_sentence = ' '.join(list_sentence_token)\n",
    "            catch_entity_threshold_loop = catch_entity_threshold_loop + 1\n",
    "            # print(\"output sentence: {0}\".format(normalized_input_sentence))\n",
    "    return result_entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✪ ✪ ✪ diễn ra hiến máu tình nguyện đợt ii ở hội trường lớn đh ngân hàng vậy\n",
      "{'intent': 'request', 'inform_slots': {'time': ['hội trường'], 'name_activity': ['hiến máu tình nguyện đợt ii'], 'holder': ['đh ngân hàng']}, 'request_slots': {'time': 'UNK'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "CONSTANT_FILE_PATH = 'constants.json'\n",
    "with open(CONSTANT_FILE_PATH) as f:\n",
    "    constants = json.load(f)\n",
    "\n",
    "file_path_dict = constants['db_file_paths']\n",
    "DATABASE_FILE_PATH = file_path_dict['database']\n",
    "\n",
    "database= json.load(open(DATABASE_FILE_PATH,encoding='utf-8'))\n",
    "state_tracker = StateTracker(database, constants)\n",
    "dqn_agent = DQNAgent(state_tracker.get_state_size(), constants)\n",
    "#TEST\n",
    "if __name__ == '__main__':\n",
    "    print(process_message_to_user_request(\"khi nào thì diễn ra hiến máu tình nguyện đợt ii ở hội trường lớn đh ngân hàng vậy\",state_tracker))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('list_constants.json','r') as constant_file:\n",
    "#     constant = json.load(constant_file)\n",
    "#     list_pattern_time = constant['list_pattern_time']\n",
    "#     res = []\n",
    "#     for pattern in list_pattern_time:\n",
    "#         if \"chủ nhật\" not in pattern:\n",
    "#             res.append(pattern)\n",
    "            \n",
    "#     constant['list_pattern_time'] = res\n",
    "#     with open('list_constants_new.json', 'w+') as new_constant_file:\n",
    "#         json.dump(constant,new_constant_file,ensure_ascii=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_data/real_db_494.json','r') as db_file:\n",
    "    list_data = json.load(db_file)\n",
    "    for data in list_data:\n",
    "        for key, list_value in data.items():\n",
    "            if key != '_id':\n",
    "                list_value = [compound2unicode(value).lower() for value in list_value]\n",
    "                data[key] = list(set(list_value))\n",
    "    with open('temp_data/real_db_494_final.json','w+') as new_db_file:\n",
    "        json.dump(list_data,new_db_file,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DuongCao",
   "language": "python",
   "name": "duongcao2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
