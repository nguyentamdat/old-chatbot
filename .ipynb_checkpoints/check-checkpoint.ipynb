{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"Quận 1\",\"Quận 1\",\"Quận 1\",\"Quận 1\",\"Quận 1\",\"Quận 1\",\"Quận 1\",\"Quận 1\",\"Quận 1\",\"Quận 1\",\"Quận 12\",\"Quận 12\",\"Quận 12\",\"Quận 12\",\"Quận 12\",\"Quận 12\",\"Quận 12\",\"Quận 12\",\"Quận 12\",\"Quận 12\",\"Quận 12\",\"Quận Thủ Đức\",\"Quận Thủ Đức\",\"Quận Thủ Đức\",\"Quận Thủ Đức\",\"Quận Thủ Đức\",\"Quận Thủ Đức\",\"Quận Thủ Đức\",\"Quận Thủ Đức\",\"Quận Thủ Đức\",\"Quận Thủ Đức\",\"Quận Thủ Đức\",\"Quận Thủ Đức\",\"Quận 9\",\"Quận 9\",\"Quận 9\",\"Quận 9\",\"Quận 9\",\"Quận 9\",\"Quận 9\",\"Quận 9\",\"Quận 9\",\"Quận 9\",\"Quận 9\",\"Quận 9\",\"Quận 9\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Gò Vấp\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Bình Thạnh\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Bình\",\"Quận Tân Phú\",\"Quận Tân Phú\",\"Quận Tân Phú\",\"Quận Tân Phú\",\"Quận Tân Phú\",\"Quận Tân Phú\",\"Quận Tân Phú\",\"Quận Tân Phú\",\"Quận Tân Phú\",\"Quận Tân Phú\",\"Quận Tân Phú\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận Phú Nhuận\",\"Quận 2\",\"Quận 2\",\"Quận 2\",\"Quận 2\",\"Quận 2\",\"Quận 2\",\"Quận 2\",\"Quận 2\",\"Quận 2\",\"Quận 2\",\"Quận 2\",\"Quận 3\",\"Quận 3\",\"Quận 3\",\"Quận 3\",\"Quận 3\",\"Quận 3\",\"Quận 3\",\"Quận 3\",\"Quận 3\",\"Quận 3\",\"Quận 3\",\"Quận 3\",\"Quận 3\",\"Quận 3\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 10\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 11\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 4\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 5\",\"Quận 6\",\"Quận 6\",\"Quận 6\",\"Quận 6\",\"Quận 6\",\"Quận 6\",\"Quận 6\",\"Quận 6\",\"Quận 6\",\"Quận 6\",\"Quận 6\",\"Quận 6\",\"Quận 6\",\"Quận 6\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận 8\",\"Quận Bình Tân\",\"Quận Bình Tân\",\"Quận Bình Tân\",\"Quận Bình Tân\",\"Quận Bình Tân\",\"Quận Bình Tân\",\"Quận Bình Tân\",\"Quận Bình Tân\",\"Quận Bình Tân\",\"Quận Bình Tân\",\"Quận 7\",\"Quận 7\",\"Quận 7\",\"Quận 7\",\"Quận 7\",\"Quận 7\",\"Quận 7\",\"Quận 7\",\"Quận 7\",\"Quận 7\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Củ Chi\",\"Huyện Hóc Môn\",\"Huyện Hóc Môn\",\"Huyện Hóc Môn\",\"Huyện Hóc Môn\",\"Huyện Hóc Môn\",\"Huyện Hóc Môn\",\"Huyện Hóc Môn\",\"Huyện Hóc Môn\",\"Huyện Hóc Môn\",\"Huyện Hóc Môn\",\"Huyện Hóc Môn\",\"Huyện Hóc Môn\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Bình Chánh\",\"Huyện Nhà Bè\",\"Huyện Nhà Bè\",\"Huyện Nhà Bè\",\"Huyện Nhà Bè\",\"Huyện Nhà Bè\",\"Huyện Nhà Bè\",\"Huyện Nhà Bè\",\"Huyện Cần Giờ\",\"Huyện Cần Giờ\",\"Huyện Cần Giờ\",\"Huyện Cần Giờ\",\"Huyện Cần Giờ\",\"Huyện Cần Giờ\",\"Xã Lý Nhơn\"]\n",
    "a = [x.lower() for x in a]\n",
    "a = list(set(a))\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_=[\"mình được\",\"tôi được\",\"em được\",\"nơi\", \"tại đâu\", \"tại chỗ nào\", \"ở đâu\", \"chỗ nào\", \"tại đâu\", \"khu nào\", \"địa điểm nào\", \"chỗ diễn ra\", \"chỗ đâu\", \"khu tổ chức\", \"là tập trung\", \"là diễn ra\", \"ấp nào\", \"phường nào\", \"xã nào\", \"quận nào\", \"huyện nào\", \"thành phố nào\", \"tỉnh nào\", \"đường nào\", \"đường gì\", \"số mấy\", \"địa chỉ nào\", \"địa chỉ\", \"tên đường\", \"số nhà\", \"phường mấy\", \"quận mấy\", \"số mấy\", \"loại hoạt động\", \"loại nào\", \"loại gì\", \"kiểu hoạt động\", \"kiểu gì\", \"kiểu nào\", \"tên gì\", \"tên là gì\", \"tên hoạt động là gì\", \"tên hoạt động\", \"là tháng\", \"là ngày\", \"là vào\", \"tháng mấy\", \"thứ mấy\", \"là diễn ra vào\", \"ngày mấy\", \"khi nào\", \"lúc nào\", \"thời gian nào\", \"ngày nào\", \"ngày bao nhiêu\", \"giờ nào\", \"giờ bao nhiêu\", \"mấy giờ\", \"mấy h \", \"thời gian\", \"ban tổ chức\", \"btc\", \"ai tổ chức\", \"đơn vị nào tổ chức\", \"đơn vị tổ chức\", \"trường nào tổ chức\", \"clb nào tổ chức\", \"câu lạc bộ nào tổ chức\", \"người tổ chức\", \"tổ chức hả\", \"tổ chức phải không\", \"tổ chức đúng không\", \"có được drl\", \"có được\", \"được thứ gì\", \"bao nhiêu tiền\", \"thưởng cái gì\", \"được lợi gì\", \"bao nhiêu\", \"có ích gì\", \"được công tác xã hội\", \"được\", \"được thưởng gì\", \"được gì\", \"được cái gì\", \"có lợi gì\", \"lợi ích\", \"phần thưởng\", \"được quà gì\", \"tặng gì\", \"được bao nhiêu\", \"có được\", \"có được ngày công tác xã hội\", \"có được\", \"có được tặng\", \"có được thưởng\", \"có được cho\", \" hả \", \"chứ\", \"có biết\", \"phải không\", \"là sao\", \"khi nào\", \"nơi nào\", \"không ạ\", \"k ạ\", \"là sao\", \"nữa vậy\", \"chưa á\", \"ko ạ\", \"sao ạ\", \"chưa ạ\", \"sao vậy\", \"không vậy\", \"k vậy\", \"ko vậy\", \"chưa vậy\", \" nhỉ \", \" ai\", \" ai \", \"ở đâu\", \"ở mô\", \"đi đâu\", \"bao giờ\", \"bao lâu\", \"khi nào\", \"lúc nào\", \"hồi nào\", \"vì sao\", \"tại sao\", \"thì sao\", \"làm sao\", \"như nào\", \"thế nào\", \"cái chi\", \"gì\", \"bao nhiêu\", \"?\", \" hả \", \"được không\", \"được k\", \"được ko\", \"vậy ạ\", \"nào vậy\", \"nào thế\", \"nữa không\", \"đúng không\", \"đúng k\", \"đúng ko\", \"nữa k\", \"nữa ko\", \"nào ấy\", \"nào ạ\", \"nào\", \"gì\", \"mấy\", \"đâu\", \"chứ\", \"thế\", \"sao bạn\", \"sao cậu\", \"sao ad\", \"sao anh\", \"sao chị\", \"sao admin\", \"sao em\", \"sao mày\", \"sao bot\", \"còn không\", \"mình muốn hỏi\", \"cho mình hỏi\", \"mình cần hỏi\", \"mình muốn hỏi thông tin\", \"cho mình hỏi thông tin\", \"mình cần hỏi thông tin\", \"mình muốn biết\", \"cho mình biết\", \"mình cần biết\", \"mình muốn biết thông tin\", \"cho mình biết thông tin\", \"mình cần biết thông tin\", \"mình muốn xin\", \"cho mình xin\", \"mình cần xin\", \"mình muốn xin thông tin\", \"cho mình xin thông tin\", \"mình cần xin thông tin\", \"mình muốn tham gia\", \"cho mình tham gia\", \"mình cần tham gia\", \"mình muốn tham gia thông tin\", \"cho mình tham gia thông tin\", \"mình cần tham gia thông tin\", \"tôi muốn hỏi\", \"cho tôi hỏi\", \"tôi cần hỏi\", \"tôi muốn hỏi thông tin\", \"cho tôi hỏi thông tin\", \"tôi cần hỏi thông tin\", \"tôi muốn biết\", \"cho tôi biết\", \"tôi cần biết\", \"tôi muốn biết thông tin\", \"cho tôi biết thông tin\", \"tôi cần biết thông tin\", \"tôi muốn xin\", \"cho tôi xin\", \"tôi cần xin\", \"tôi muốn xin thông tin\", \"cho tôi xin thông tin\", \"tôi cần xin thông tin\", \"tôi muốn tham gia\", \"cho tôi tham gia\", \"tôi cần tham gia\", \"tôi muốn tham gia thông tin\", \"cho tôi tham gia thông tin\", \"tôi cần tham gia thông tin\", \"tớ muốn hỏi\", \"cho tớ hỏi\", \"tớ cần hỏi\", \"tớ muốn hỏi thông tin\", \"cho tớ hỏi thông tin\", \"tớ cần hỏi thông tin\", \"tớ muốn biết\", \"cho tớ biết\", \"tớ cần biết\", \"tớ muốn biết thông tin\", \"cho tớ biết thông tin\", \"tớ cần biết thông tin\", \"tớ muốn xin\", \"cho tớ xin\", \"tớ cần xin\", \"tớ muốn xin thông tin\", \"cho tớ xin thông tin\", \"tớ cần xin thông tin\", \"tớ muốn tham gia\", \"cho tớ tham gia\", \"tớ cần tham gia\", \"tớ muốn tham gia thông tin\", \"cho tớ tham gia thông tin\", \"tớ cần tham gia thông tin\", \"tao muốn hỏi\", \"cho tao hỏi\", \"tao cần hỏi\", \"tao muốn hỏi thông tin\", \"cho tao hỏi thông tin\", \"tao cần hỏi thông tin\", \"tao muốn biết\", \"cho tao biết\", \"tao cần biết\", \"tao muốn biết thông tin\", \"cho tao biết thông tin\", \"tao cần biết thông tin\", \"tao muốn xin\", \"cho tao xin\", \"tao cần xin\", \"tao muốn xin thông tin\", \"cho tao xin thông tin\", \"tao cần xin thông tin\", \"tao muốn tham gia\", \"cho tao tham gia\", \"tao cần tham gia\", \"tao muốn tham gia thông tin\", \"cho tao tham gia thông tin\", \"tao cần tham gia thông tin\", \"tui muốn hỏi\", \"cho tui hỏi\", \"tui cần hỏi\", \"tui muốn hỏi thông tin\", \"cho tui hỏi thông tin\", \"tui cần hỏi thông tin\", \"tui muốn biết\", \"cho tui biết\", \"tui cần biết\", \"tui muốn biết thông tin\", \"cho tui biết thông tin\", \"tui cần biết thông tin\", \"tui muốn xin\", \"cho tui xin\", \"tui cần xin\", \"tui muốn xin thông tin\", \"cho tui xin thông tin\", \"tui cần xin thông tin\", \"tui muốn tham gia\", \"cho tui tham gia\", \"tui cần tham gia\", \"tui muốn tham gia thông tin\", \"cho tui tham gia thông tin\", \"tui cần tham gia thông tin\", \"anh muốn hỏi\", \"cho anh hỏi\", \"anh cần hỏi\", \"anh muốn hỏi thông tin\", \"cho anh hỏi thông tin\", \"anh cần hỏi thông tin\", \"anh muốn biết\", \"cho anh biết\", \"anh cần biết\", \"anh muốn biết thông tin\", \"cho anh biết thông tin\", \"anh cần biết thông tin\", \"anh muốn xin\", \"cho anh xin\", \"anh cần xin\", \"anh muốn xin thông tin\", \"cho anh xin thông tin\", \"anh cần xin thông tin\", \"anh muốn tham gia\", \"cho anh tham gia\", \"anh cần tham gia\", \"anh muốn tham gia thông tin\", \"cho anh tham gia thông tin\", \"anh cần tham gia thông tin\", \"em muốn hỏi\", \"cho em hỏi\", \"em cần hỏi\", \"em muốn hỏi thông tin\", \"cho em hỏi thông tin\", \"em cần hỏi thông tin\", \"em muốn biết\", \"cho em biết\", \"em cần biết\", \"em muốn biết thông tin\", \"cho em biết thông tin\", \"em cần biết thông tin\", \"em muốn xin\", \"cho em xin\", \"em cần xin\", \"em muốn xin thông tin\", \"cho em xin thông tin\", \"em cần xin thông tin\", \"em muốn tham gia\", \"cho em tham gia\", \"em cần tham gia\", \"em muốn tham gia thông tin\", \"cho em tham gia thông tin\", \"em cần tham gia thông tin\", \"chứ mình\", \"mình muốn được hỏi\", \"mình muốn được tư vấn\", \"mình cần thông tin\", \"mình muốn thông tin\", \"gửi mình\", \"chỉ mình\", \"chỉ giúp mình\", \"mình muốn được hỏi thông tin\", \"chứ tôi\", \"tôi muốn được hỏi\", \"tôi muốn được tư vấn\", \"tôi cần thông tin\", \"tôi muốn thông tin\", \"gửi tôi\", \"chỉ tôi\", \"chỉ giúp tôi\", \"tôi muốn được hỏi thông tin\", \"chứ tớ\", \"tớ muốn được hỏi\", \"tớ muốn được tư vấn\", \"tớ cần thông tin\", \"tớ muốn thông tin\", \"gửi tớ\", \"chỉ tớ\", \"chỉ giúp tớ\", \"tớ muốn được hỏi thông tin\", \"chứ tao\", \"tao muốn được hỏi\", \"tao muốn được tư vấn\", \"tao cần thông tin\", \"tao muốn thông tin\", \"gửi tao\", \"chỉ tao\", \"chỉ giúp tao\", \"tao muốn được hỏi thông tin\", \"chứ tui\", \"tui muốn được hỏi\", \"tui muốn được tư vấn\", \"tui cần thông tin\", \"tui muốn thông tin\", \"gửi tui\", \"chỉ tui\", \"chỉ giúp tui\", \"tui muốn được hỏi thông tin\", \"chứ anh\", \"anh muốn được hỏi\", \"anh muốn được tư vấn\", \"anh cần thông tin\", \"anh muốn thông tin\", \"gửi anh\", \"chỉ anh\", \"chỉ giúp anh\", \"anh muốn được hỏi thông tin\", \"chứ em\", \"em muốn được hỏi\", \"em muốn được tư vấn\", \"em cần thông tin\", \"em muốn thông tin\", \"gửi em\", \"chỉ em\", \"chỉ giúp em\", \"em muốn được hỏi thông tin\", \"cho hỏi\", \"cho biết\", \"cho xin\", \"cho tham gia\", \"cho mình\", \"gửi mình\", \"mình cần\", \"mình muốn\", \"mình định\", \"cho mình thông tin\", \"gửi mình thông tin\", \"chỉ giúp mình thông tin\", \"mình cần\", \"mình muốn \", \"mình định\", \"cho tôi\", \"gửi tôi\", \"tôi cần\", \"tôi muốn\", \"tôi định\", \"cho tôi thông tin\", \"gửi tôi thông tin\", \"chỉ giúp tôi thông tin\", \"tôi cần\", \"tôi muốn \", \"tôi định\", \"cho tớ\", \"gửi tớ\", \"tớ cần\", \"tớ muốn\", \"tớ định\", \"cho tớ thông tin\", \"gửi tớ thông tin\", \"chỉ giúp tớ thông tin\", \"tớ cần\", \"tớ muốn \", \"tớ định\", \"cho tao\", \"gửi tao\", \"tao cần\", \"tao muốn\", \"tao định\", \"cho tao thông tin\", \"gửi tao thông tin\", \"chỉ giúp tao thông tin\", \"tao cần\", \"tao muốn \", \"tao định\", \"cho tui\", \"gửi tui\", \"tui cần\", \"tui muốn\", \"tui định\", \"cho tui thông tin\", \"gửi tui thông tin\", \"chỉ giúp tui thông tin\", \"tui cần\", \"tui muốn \", \"tui định\", \"cho anh\", \"gửi anh\", \"anh cần\", \"anh muốn\", \"anh định\", \"cho anh thông tin\", \"gửi anh thông tin\", \"chỉ giúp anh thông tin\", \"anh cần\", \"anh muốn \", \"anh định\", \"cho em\", \"gửi em\", \"em cần\", \"em muốn\", \"em định\", \"cho em thông tin\", \"gửi em thông tin\", \"chỉ giúp em thông tin\", \"em cần\", \"em muốn \", \"em định\",\"vào ngày\",\"hoạt động\",\"vào lúc\",\"thì\",\"khi\",\"lúc\"]\n",
    "list_.sort(reverse = True)\n",
    "# print(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs_length_ta(x, y):\n",
    "    m = len(x)\n",
    "    n = len(y)\n",
    "    c = [[0 for x in range(n+1)] for y in range(m+1)] \n",
    "    b = [['' for x in range(n+1)] for y in range(m+1)] \n",
    "    \n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            if x[i-1] == y[j-1]:\n",
    "                c[i][j] = c[i-1][j-1] + 1\n",
    "                b[i][j] = '↖️'\n",
    "            elif c[i - 1][j] >= c[i][j-1]:\n",
    "                c[i][j] = c[i - 1][j] \n",
    "                b[i][j] = '⬆️'\n",
    "            else:\n",
    "                c[i][j] = c[i][j-1] \n",
    "                b[i][j] = '⬅️'\n",
    "    max_common_length = c[-1][-1]\n",
    "    i = m\n",
    "    j = n\n",
    "    pointer = c[i][j]\n",
    "    result_index = []\n",
    "    i_max = 0\n",
    "    max_length_in_sentence = 0\n",
    "    while pointer != 0:\n",
    "        if pointer == max_common_length and b[i][j]=='⬆️':\n",
    "            i = i-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer == max_common_length and b[i][j]=='⬅️':\n",
    "            j = j-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer == max_common_length and b[i][j]=='↖️':\n",
    "            result_index.append(i)\n",
    "            i = i-1\n",
    "            j = j-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer != max_common_length and b[i][j]=='⬆️':\n",
    "            result_index.append(i)\n",
    "            i = i-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer != max_common_length and b[i][j]=='⬅️':\n",
    "            result_index.append(i)\n",
    "            j = j-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer != max_common_length and b[i][j]=='↖️':\n",
    "            result_index.append(i)\n",
    "            j = j-1\n",
    "            i = i-1\n",
    "            pointer = c[i][j]\n",
    "#     print(result_index)\n",
    "#     print(max_common_length)\n",
    "    if result_index != [] :\n",
    "        i_max = result_index[0]-1\n",
    "        max_length_in_sentence = result_index[0] -result_index[-1] +1\n",
    "    \n",
    "    return max_common_length,max_length_in_sentence,i_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7, 25)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs_length_ta(\"✪ ✪ ✪ đối tượng ✪ ✪ của ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪ là ✪ ✪ ✪ ✪ ✪ bạn\".split(\" \"),\"đặc biệt là các bạn nữ\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound2unicode(text):\n",
    "  #https://gist.github.com/redphx/9320735`\n",
    "  text = text.replace(\"\\u0065\\u0309\", \"\\u1EBB\")    # ẻ\n",
    "  text = text.replace(\"\\u0065\\u0301\", \"\\u00E9\")    # é\n",
    "  text = text.replace(\"\\u0065\\u0300\", \"\\u00E8\")    # è\n",
    "  text = text.replace(\"\\u0065\\u0323\", \"\\u1EB9\")    # ẹ\n",
    "  text = text.replace(\"\\u0065\\u0303\", \"\\u1EBD\")    # ẽ\n",
    "  text = text.replace(\"\\u00EA\\u0309\", \"\\u1EC3\")    # ể\n",
    "  text = text.replace(\"\\u00EA\\u0301\", \"\\u1EBF\")    # ế\n",
    "  text = text.replace(\"\\u00EA\\u0300\", \"\\u1EC1\")    # ề\n",
    "  text = text.replace(\"\\u00EA\\u0323\", \"\\u1EC7\")    # ệ\n",
    "  text = text.replace(\"\\u00EA\\u0303\", \"\\u1EC5\")    # ễ\n",
    "  text = text.replace(\"\\u0079\\u0309\", \"\\u1EF7\")    # ỷ\n",
    "  text = text.replace(\"\\u0079\\u0301\", \"\\u00FD\")    # ý\n",
    "  text = text.replace(\"\\u0079\\u0300\", \"\\u1EF3\")    # ỳ\n",
    "  text = text.replace(\"\\u0079\\u0323\", \"\\u1EF5\")    # ỵ\n",
    "  text = text.replace(\"\\u0079\\u0303\", \"\\u1EF9\")    # ỹ\n",
    "  text = text.replace(\"\\u0075\\u0309\", \"\\u1EE7\")    # ủ\n",
    "  text = text.replace(\"\\u0075\\u0301\", \"\\u00FA\")    # ú\n",
    "  text = text.replace(\"\\u0075\\u0300\", \"\\u00F9\")    # ù\n",
    "  text = text.replace(\"\\u0075\\u0323\", \"\\u1EE5\")    # ụ\n",
    "  text = text.replace(\"\\u0075\\u0303\", \"\\u0169\")    # ũ\n",
    "  text = text.replace(\"\\u01B0\\u0309\", \"\\u1EED\")    # ử\n",
    "  text = text.replace(\"\\u01B0\\u0301\", \"\\u1EE9\")    # ứ\n",
    "  text = text.replace(\"\\u01B0\\u0300\", \"\\u1EEB\")    # ừ\n",
    "  text = text.replace(\"\\u01B0\\u0323\", \"\\u1EF1\")    # ự\n",
    "  text = text.replace(\"\\u01B0\\u0303\", \"\\u1EEF\")    # ữ\n",
    "  text = text.replace(\"\\u0069\\u0309\", \"\\u1EC9\")    # ỉ\n",
    "  text = text.replace(\"\\u0069\\u0301\", \"\\u00ED\")    # í\n",
    "  text = text.replace(\"\\u0069\\u0300\", \"\\u00EC\")    # ì\n",
    "  text = text.replace(\"\\u0069\\u0323\", \"\\u1ECB\")    # ị\n",
    "  text = text.replace(\"\\u0069\\u0303\", \"\\u0129\")    # ĩ\n",
    "  text = text.replace(\"\\u006F\\u0309\", \"\\u1ECF\")    # ỏ\n",
    "  text = text.replace(\"\\u006F\\u0301\", \"\\u00F3\")    # ó\n",
    "  text = text.replace(\"\\u006F\\u0300\", \"\\u00F2\")    # ò\n",
    "  text = text.replace(\"\\u006F\\u0323\", \"\\u1ECD\")    # ọ\n",
    "  text = text.replace(\"\\u006F\\u0303\", \"\\u00F5\")    # õ\n",
    "  text = text.replace(\"\\u01A1\\u0309\", \"\\u1EDF\")    # ở\n",
    "  text = text.replace(\"\\u01A1\\u0301\", \"\\u1EDB\")    # ớ\n",
    "  text = text.replace(\"\\u01A1\\u0300\", \"\\u1EDD\")    # ờ\n",
    "  text = text.replace(\"\\u01A1\\u0323\", \"\\u1EE3\")    # ợ\n",
    "  text = text.replace(\"\\u01A1\\u0303\", \"\\u1EE1\")    # ỡ\n",
    "  text = text.replace(\"\\u00F4\\u0309\", \"\\u1ED5\")    # ổ\n",
    "  text = text.replace(\"\\u00F4\\u0301\", \"\\u1ED1\")    # ố\n",
    "  text = text.replace(\"\\u00F4\\u0300\", \"\\u1ED3\")    # ồ\n",
    "  text = text.replace(\"\\u00F4\\u0323\", \"\\u1ED9\")    # ộ\n",
    "  text = text.replace(\"\\u00F4\\u0303\", \"\\u1ED7\")    # ỗ\n",
    "  text = text.replace(\"\\u0061\\u0309\", \"\\u1EA3\")    # ả\n",
    "  text = text.replace(\"\\u0061\\u0301\", \"\\u00E1\")    # á\n",
    "  text = text.replace(\"\\u0061\\u0300\", \"\\u00E0\")    # à\n",
    "  text = text.replace(\"\\u0061\\u0323\", \"\\u1EA1\")    # ạ\n",
    "  text = text.replace(\"\\u0061\\u0303\", \"\\u00E3\")    # ã\n",
    "  text = text.replace(\"\\u0103\\u0309\", \"\\u1EB3\")    # ẳ\n",
    "  text = text.replace(\"\\u0103\\u0301\", \"\\u1EAF\")    # ắ\n",
    "  text = text.replace(\"\\u0103\\u0300\", \"\\u1EB1\")    # ằ\n",
    "  text = text.replace(\"\\u0103\\u0323\", \"\\u1EB7\")    # ặ\n",
    "  text = text.replace(\"\\u0103\\u0303\", \"\\u1EB5\")    # ẵ\n",
    "  text = text.replace(\"\\u00E2\\u0309\", \"\\u1EA9\")    # ẩ\n",
    "  text = text.replace(\"\\u00E2\\u0301\", \"\\u1EA5\")    # ấ\n",
    "  text = text.replace(\"\\u00E2\\u0300\", \"\\u1EA7\")    # ầ\n",
    "  text = text.replace(\"\\u00E2\\u0323\", \"\\u1EAD\")    # ậ\n",
    "  text = text.replace(\"\\u00E2\\u0303\", \"\\u1EAB\")    # ẫ\n",
    "  text = text.replace(\"\\u0045\\u0309\", \"\\u1EBA\")    # Ẻ\n",
    "  text = text.replace(\"\\u0045\\u0301\", \"\\u00C9\")    # É\n",
    "  text = text.replace(\"\\u0045\\u0300\", \"\\u00C8\")    # È\n",
    "  text = text.replace(\"\\u0045\\u0323\", \"\\u1EB8\")    # Ẹ\n",
    "  text = text.replace(\"\\u0045\\u0303\", \"\\u1EBC\")    # Ẽ\n",
    "  text = text.replace(\"\\u00CA\\u0309\", \"\\u1EC2\")    # Ể\n",
    "  text = text.replace(\"\\u00CA\\u0301\", \"\\u1EBE\")    # Ế\n",
    "  text = text.replace(\"\\u00CA\\u0300\", \"\\u1EC0\")    # Ề\n",
    "  text = text.replace(\"\\u00CA\\u0323\", \"\\u1EC6\")    # Ệ\n",
    "  text = text.replace(\"\\u00CA\\u0303\", \"\\u1EC4\")    # Ễ\n",
    "  text = text.replace(\"\\u0059\\u0309\", \"\\u1EF6\")    # Ỷ\n",
    "  text = text.replace(\"\\u0059\\u0301\", \"\\u00DD\")    # Ý\n",
    "  text = text.replace(\"\\u0059\\u0300\", \"\\u1EF2\")    # Ỳ\n",
    "  text = text.replace(\"\\u0059\\u0323\", \"\\u1EF4\")    # Ỵ\n",
    "  text = text.replace(\"\\u0059\\u0303\", \"\\u1EF8\")    # Ỹ\n",
    "  text = text.replace(\"\\u0055\\u0309\", \"\\u1EE6\")    # Ủ\n",
    "  text = text.replace(\"\\u0055\\u0301\", \"\\u00DA\")    # Ú\n",
    "  text = text.replace(\"\\u0055\\u0300\", \"\\u00D9\")    # Ù\n",
    "  text = text.replace(\"\\u0055\\u0323\", \"\\u1EE4\")    # Ụ\n",
    "  text = text.replace(\"\\u0055\\u0303\", \"\\u0168\")    # Ũ\n",
    "  text = text.replace(\"\\u01AF\\u0309\", \"\\u1EEC\")    # Ử\n",
    "  text = text.replace(\"\\u01AF\\u0301\", \"\\u1EE8\")    # Ứ\n",
    "  text = text.replace(\"\\u01AF\\u0300\", \"\\u1EEA\")    # Ừ\n",
    "  text = text.replace(\"\\u01AF\\u0323\", \"\\u1EF0\")    # Ự\n",
    "  text = text.replace(\"\\u01AF\\u0303\", \"\\u1EEE\")    # Ữ\n",
    "  text = text.replace(\"\\u0049\\u0309\", \"\\u1EC8\")    # Ỉ\n",
    "  text = text.replace(\"\\u0049\\u0301\", \"\\u00CD\")    # Í\n",
    "  text = text.replace(\"\\u0049\\u0300\", \"\\u00CC\")    # Ì\n",
    "  text = text.replace(\"\\u0049\\u0323\", \"\\u1ECA\")    # Ị\n",
    "  text = text.replace(\"\\u0049\\u0303\", \"\\u0128\")    # Ĩ\n",
    "  text = text.replace(\"\\u004F\\u0309\", \"\\u1ECE\")    # Ỏ\n",
    "  text = text.replace(\"\\u004F\\u0301\", \"\\u00D3\")    # Ó\n",
    "  text = text.replace(\"\\u004F\\u0300\", \"\\u00D2\")    # Ò\n",
    "  text = text.replace(\"\\u004F\\u0323\", \"\\u1ECC\")    # Ọ\n",
    "  text = text.replace(\"\\u004F\\u0303\", \"\\u00D5\")    # Õ\n",
    "  text = text.replace(\"\\u01A0\\u0309\", \"\\u1EDE\")    # Ở\n",
    "  text = text.replace(\"\\u01A0\\u0301\", \"\\u1EDA\")    # Ớ\n",
    "  text = text.replace(\"\\u01A0\\u0300\", \"\\u1EDC\")    # Ờ\n",
    "  text = text.replace(\"\\u01A0\\u0323\", \"\\u1EE2\")    # Ợ\n",
    "  text = text.replace(\"\\u01A0\\u0303\", \"\\u1EE0\")    # Ỡ\n",
    "  text = text.replace(\"\\u00D4\\u0309\", \"\\u1ED4\")    # Ổ\n",
    "  text = text.replace(\"\\u00D4\\u0301\", \"\\u1ED0\")    # Ố\n",
    "  text = text.replace(\"\\u00D4\\u0300\", \"\\u1ED2\")    # Ồ\n",
    "  text = text.replace(\"\\u00D4\\u0323\", \"\\u1ED8\")    # Ộ\n",
    "  text = text.replace(\"\\u00D4\\u0303\", \"\\u1ED6\")    # Ỗ\n",
    "  text = text.replace(\"\\u0041\\u0309\", \"\\u1EA2\")    # Ả\n",
    "  text = text.replace(\"\\u0041\\u0301\", \"\\u00C1\")    # Á\n",
    "  text = text.replace(\"\\u0041\\u0300\", \"\\u00C0\")    # À\n",
    "  text = text.replace(\"\\u0041\\u0323\", \"\\u1EA0\")    # Ạ\n",
    "  text = text.replace(\"\\u0041\\u0303\", \"\\u00C3\")    # Ã\n",
    "  text = text.replace(\"\\u0102\\u0309\", \"\\u1EB2\")    # Ẳ\n",
    "  text = text.replace(\"\\u0102\\u0301\", \"\\u1EAE\")    # Ắ\n",
    "  text = text.replace(\"\\u0102\\u0300\", \"\\u1EB0\")    # Ằ\n",
    "  text = text.replace(\"\\u0102\\u0323\", \"\\u1EB6\")    # Ặ\n",
    "  text = text.replace(\"\\u0102\\u0303\", \"\\u1EB4\")    # Ẵ\n",
    "  text = text.replace(\"\\u00C2\\u0309\", \"\\u1EA8\")    # Ẩ\n",
    "  text = text.replace(\"\\u00C2\\u0301\", \"\\u1EA4\")    # Ấ\n",
    "  text = text.replace(\"\\u00C2\\u0300\", \"\\u1EA6\")    # Ầ\n",
    "  text = text.replace(\"\\u00C2\\u0323\", \"\\u1EAC\")    # Ậ\n",
    "  text = text.replace(\"\\u00C2\\u0303\", \"\\u1EAA\")    # Ẫ\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name_activity', 'type_activity', 'holder', 'time', 'name_place', 'address', 'reward', 'contact', 'register', 'works', 'joiner'])\n",
      "------------------------type activity\n",
      "tuyển hỗ trợ viên\n",
      "hỗ trợ chương trình\n",
      "hỗ trợ triễn lãm\n",
      "buổi hỗ trợ\n",
      "tuyển ctv hỗ trợ\n",
      "hỗ trợ\n",
      "hỗ trợ gây quỹ\n"
     ]
    }
   ],
   "source": [
    "with open('real_dict_2000_new_only_delete_question_noti_new_and_space_newest.json','r') as dict_file:\n",
    "    real_dict = json.load(dict_file)\n",
    "    print(real_dict.keys())\n",
    "    list_holder = real_dict['holder']\n",
    "    list_name_place = real_dict['name_place']\n",
    "    list_name_activity = real_dict['name_activity']\n",
    "    list_joiner = real_dict['joiner']\n",
    "    list_type_activity = real_dict['type_activity']\n",
    "    list_reward = real_dict['reward']\n",
    "    list_works = real_dict['works']\n",
    "    \n",
    "    res = []\n",
    "\n",
    "#     print(\"------------------------joiner\")\n",
    "#     for joiner in list_joiner:\n",
    "#         if compound2unicode(\"bạn\") in compound2unicode(joiner):\n",
    "#             print(joiner)\n",
    "#     print(\"------------------------works\")\n",
    "#     for works in list_works:\n",
    "#         if compound2unicode(\"hoạt động\") in compound2unicode(works):\n",
    "#             print(works)\n",
    "            \n",
    "#     print(\"------------------------reward\")\n",
    "#     for reward in list_reward:\n",
    "# #         if \"khoa học và công nghệ\" in reward:\n",
    "#         print(reward)\n",
    "            \n",
    "    print(\"------------------------type activity\")\n",
    "    for type_activity in list_type_activity:\n",
    "        if compound2unicode(\"hỗ trợ\") in compound2unicode(type_activity):\n",
    "            print(type_activity)\n",
    "            \n",
    "#     print(\"------------------------holder\")\n",
    "#     for holder in list_holder:\n",
    "#         if compound2unicode(\"bách khoa\") in compound2unicode(holder):\n",
    "#             print(holder)\n",
    "            \n",
    "#     print(\"------------------------name_place\")\n",
    "#     for name_place in list_name_place:\n",
    "#         if compound2unicode(\"bách khoa\") in compound2unicode(name_place):\n",
    "#             print(name_place)\n",
    "#     print(\"------------------------name_activity\")\n",
    "#     for name_activity in list_name_activity:\n",
    "#         if \"bách khoa\" in name_activity:\n",
    "#             print(name_activity)\n",
    "#     for joiner in list_joiner:\n",
    "#         print(joiner)\n",
    "#         if name_activity.find(\"chương trình\") == 0:\n",
    "#             res.append(name_activity.replace(\"chương trình \",\"\"))\n",
    "#         else:\n",
    "#             res.append(name_activity)\n",
    "# #     print(res)\n",
    "#     real_dict['name_activity'] = res\n",
    "#     with open('real_dict_2000_new_only_delete_question_noti_new_and_space_newest.json', 'w+') as new_dict_file:\n",
    "#         json.dump(real_dict,new_dict_file,ensure_ascii=False)\n",
    "# print(list_holder)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494\n"
     ]
    }
   ],
   "source": [
    "with open('temp_data/real_db_769.json','r') as db_file:\n",
    "    list_data = json.load(db_file)\n",
    "#     print(list_data)\n",
    "    res = list_data\n",
    "    for data in list_data:\n",
    "        check_not_match_holder = False\n",
    "#         if 'name_activity' not in list(data.keys()):\n",
    "#             break\n",
    "        for holder in data['holder']:\n",
    "            if holder not in list_holder:\n",
    "                check_not_match_holder = True\n",
    "                break\n",
    "        if check_not_match_holder == True:\n",
    "            res.remove(data)\n",
    "            \n",
    "    for data in list_data:\n",
    "        check_not_match_name_place = False\n",
    "#         if 'name_activity' not in list(data.keys()):\n",
    "#             break\n",
    "        for name_place in data['name_place']:\n",
    "            if name_place not in list_name_place:\n",
    "                check_not_match_name_place = True\n",
    "                break\n",
    "        if check_not_match_name_place == True:\n",
    "            res.remove(data)\n",
    "    \n",
    "    \n",
    "    print(len(res))\n",
    "    with open('temp_data/real_db_494.json','w+') as new_db_file:\n",
    "        json.dump(res,new_db_file,ensure_ascii=False)\n",
    "    \n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\d\\d\\d\n"
     ]
    }
   ],
   "source": [
    "with open('list_constants.json','r') as constants_file:\n",
    "    list_constants = json.load(constants_file)\n",
    "    list_pattern_time = list_constants['list_pattern_time']\n",
    "    for pattern in list_pattern_time:\n",
    "        if re.findall(pattern,'305') != []:\n",
    "            print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp_agent_action_gen import *\n",
    "from message_handler import *\n",
    "from agen_response_gen import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_entity(intent,input_sentence):\n",
    "    print(\"duongcc\")\n",
    "    normalized_input_sentence = compound2unicode(input_sentence)\n",
    "    normalized_input_sentence = delete_extra_word(normalized_input_sentence,list_extra_word)\n",
    "    \n",
    "    result_entity_dict={}\n",
    "    list_order_entity_name=map_intent_to_list_order_entity_name[intent]\n",
    "    print(normalized_input_sentence)\n",
    "    if 'time' in list_order_entity_name:\n",
    "        for pattern_time in list_pattern_time:\n",
    "            if re.findall(pattern_time,normalized_input_sentence)!=[]:\n",
    "                # print(\"pattern_time :{0}\".format(pattern_time))\n",
    "                if 'time' not in result_entity_dict:\n",
    "                    result_entity_dict['time'] = delete_last_space_list(re.findall(pattern_time,normalized_input_sentence))\n",
    "                else:\n",
    "                    result_entity_dict['time'].extend(delete_last_space_list(re.findall(pattern_time,normalized_input_sentence)))\n",
    "                \n",
    "                normalized_input_sentence = re.sub(pattern_time,' '.join(['✪']*(pattern_time.count(' ')+1)),normalized_input_sentence)\n",
    "        # if 'time' in result_entity_dict:\n",
    "        #     print(result_entity_dict['time'])\n",
    "    if 'reward' in list_order_entity_name:\n",
    "        for pattern_reward in list_pattern_reward:\n",
    "            if re.findall(pattern_reward,normalized_input_sentence)!=[]:\n",
    "                print(\"pattern_reward :{0}\".format(pattern_reward))\n",
    "                if 'reward' not in result_entity_dict:\n",
    "                    result_entity_dict['reward'] = delete_last_space_list(re.findall(pattern_reward,normalized_input_sentence))\n",
    "                else:\n",
    "                    result_entity_dict['reward'].extend(delete_last_space_list(re.findall(pattern_reward,normalized_input_sentence)))\n",
    "                \n",
    "                normalized_input_sentence = re.sub(pattern_reward,' '.join(['✪']*(pattern_reward.count(' ')+1)),normalized_input_sentence)\n",
    "        # if 'reward' in result_entity_dict:\n",
    "        #     print(result_entity_dict['reward'])\n",
    "    matching_threshold = 0.0\n",
    "    longest_common_length, end_common_index = None, None\n",
    "    \n",
    "    map_entity_name_to_threshold={}\n",
    "    for entity_name in list_order_entity_name:\n",
    "        if entity_name in ['time','address']:\n",
    "            map_entity_name_to_threshold[entity_name]=1\n",
    "        elif entity_name in ['name_activity','contact','joiner','holder','type_activity','name_place']:\n",
    "            map_entity_name_to_threshold[entity_name]=2\n",
    "        elif entity_name in ['works','reward']:\n",
    "            map_entity_name_to_threshold[entity_name]=2\n",
    "        elif entity_name in ['register']:\n",
    "            map_entity_name_to_threshold[entity_name]=3\n",
    "\n",
    "\n",
    "    ordered_real_dict = OrderedDict()\n",
    "    for entity_name in map_intent_to_list_order_entity_name[intent]:\n",
    "        ordered_real_dict[entity_name] = real_dict[entity_name]\n",
    "    for entity_name, list_entity in ordered_real_dict.items():\n",
    "        # print(entity_name)\n",
    "        list_entity = [entity.lower() for entity in list_entity]\n",
    "        # print(\"input sentence: {0}\".format(normalized_input_sentence))\n",
    "        if entity_name in [\"works\",\"register\",\"reward\"]:\n",
    "            matching_threshold = 0.15\n",
    "        elif entity_name == \"joiner\":\n",
    "            matching_threshold = 0.2\n",
    "        else:\n",
    "            matching_threshold = 0.5\n",
    "        print(\"000. sentence:{0}\".format(normalized_input_sentence))\n",
    "        catch_entity_threshold_loop = 0\n",
    "        while True:\n",
    "            if catch_entity_threshold_loop > 5:\n",
    "                break\n",
    "            list_dict_longest_common_entity = find_entity_longest_common(normalized_input_sentence,list_entity,entity_name)\n",
    "#             print(list_dict_longest_common_entity)\n",
    "                #     [{'longest_common_entity_index': 0,\n",
    "                #   'longest_common_length': 3,\n",
    "                #   'end_common_index': 9}]\n",
    "            \n",
    "\n",
    "            ##find the most match longest common match (calculate by length of token match in sentence \n",
    "                                                                #/ length of entity )\n",
    "                            ##{'greatest_match_entity_index':0,'longest_common_length':3,'end_common_index':9}\n",
    "            if list_dict_longest_common_entity == []:\n",
    "                break\n",
    "            if list_dict_longest_common_entity[0]['longest_common_length'] < map_entity_name_to_threshold[entity_name] :\n",
    "                break\n",
    "            \n",
    "            list_sentence_token = normalized_input_sentence.split(' ')\n",
    "#             print(\"list_sentence_token :{0}\".format(list_sentence_token))\n",
    "            greatest_entity_index=None\n",
    "            greatest_common_length = None\n",
    "            greatest_end_common_index = None\n",
    "            max_match_entity = 0.0\n",
    "#             print(\"common entity :{0}\".format(list_dict_longest_common_entity))\n",
    "            for dict_longest_common_entity in list_dict_longest_common_entity:\n",
    "#                 print(\"0. dict_longest_common_entity: {0}\".format(dict_longest_common_entity))\n",
    "\n",
    "#                     print(\"duong\")\n",
    "#                 print(\"0.1 entity: {0}\".format(list_entity[dict_longest_common_entity['longest_common_entity_index']]))\n",
    "                longest_common_entity_index = dict_longest_common_entity['longest_common_entity_index']\n",
    "                longest_common_length = dict_longest_common_entity['longest_common_length']\n",
    "                end_common_index = dict_longest_common_entity['end_common_index']\n",
    "                \n",
    "                list_sentence_token_match = list_sentence_token[end_common_index - longest_common_length+1:end_common_index+1]\n",
    "                if entity_name == \"type_activity\":\n",
    "                    if \"ban chỉ huy\" in normalized_input_sentence or \"ban tổ chức\" in normalized_input_sentence or \"bch\" in normalized_input_sentence or \"btc\" in normalized_input_sentence:\n",
    "                        continue\n",
    "                    #nếu chỉ là các câu inform 1 entity mà câu đó không phải là câu inform tên 1 hoạt động thì không cần xét\n",
    "                    if \"inform\" not in intent or \"name_activity\" in intent:\n",
    "                        list_name_activity = ordered_real_dict['name_activity']\n",
    "                        check_in_name = False\n",
    "                        for name_activity in list_name_activity:\n",
    "                            #nếu loại hoạt động nằm trọn trong bất kì 1 tên hoạt động \n",
    "                            # thì không lấy\n",
    "                            if  name_activity.find(' '.join(list_sentence_token_match)) > 0:\n",
    "                                check_in_name = True\n",
    "                                break\n",
    "                        if check_in_name == True:\n",
    "                            continue\n",
    "                \n",
    "                if entity_name == \"holder\":\n",
    "                    # nếu holder mà trước đó có từ chỉ nơi chốn : ở, tại => không là holder mà là  \n",
    "                    # name_place\n",
    "                    if end_common_index - longest_common_length >= 0:\n",
    "                        if list_sentence_token[end_common_index - longest_common_length] in [\"ở\",\"tại\",\"trước\",\"sau\",\"trong\"]:\n",
    "                            if 'name_place' in result_entity_dict:\n",
    "            #                     result_entity_dict[entity_name].append(list_entity[greatest_entity_index])\n",
    "                                result_entity_dict['name_place'].append(' '.join(list_sentence_token_match))\n",
    "                            else:\n",
    "            #                     result_entity_dict[entity_name] = [list_entity[greatest_entity_index]]\n",
    "                                result_entity_dict['name_place'] = [' '.join(list_sentence_token_match)]\n",
    "                            list_sentence_token[end_common_index - longest_common_length +1 :end_common_index +1] = [\"✪\"]*longest_common_length\n",
    "                            normalized_input_sentence = ' '.join(list_sentence_token)\n",
    "                            continue\n",
    "\n",
    "#                 print(\"2. list_sentence_token_match : {0}\".format(list_sentence_token_match))\n",
    "                list_temp_longest_entity_token = list_entity[longest_common_entity_index].split(' ')\n",
    "#                 print(\"3. list_temp_longest_entity_token : {0}\".format(list_temp_longest_entity_token))\n",
    "                if entity_name in [\"works\",\"register\",\"reward\"]:\n",
    "#                     print(\"list_temp_longest_entity_token :{0}\".format(list_temp_longest_entity_token))\n",
    "#                     print(\"list_sentence_token_match :{0}\".format(list_sentence_token_match))\n",
    "                    _,longest_common_length_entity,end_common_index_entity = lcs_length_ta(list_temp_longest_entity_token,list_sentence_token_match)\n",
    "                    list_entity_token_match = list_temp_longest_entity_token[end_common_index_entity - longest_common_length_entity +1 :end_common_index_entity +1]\n",
    "                    score = len(list_entity_token_match)/float(len(list_temp_longest_entity_token))\n",
    "#                     print(\"list_entity_token_match: {0}\".format(list_entity_token_match))\n",
    "#                     print(\"list_temp_longest_entity_token:{0}\".format(list_temp_longest_entity_token))\n",
    "#                     print(\"score :{0}\".format(score))\n",
    "                    \n",
    "                else:\n",
    "                    score = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))\n",
    "                if score > max_match_entity:\n",
    "#                     max_match_entity = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))\n",
    "                    max_match_entity = score\n",
    "                    greatest_entity_index = longest_common_entity_index\n",
    "                    greatest_common_length = longest_common_length\n",
    "                    greatest_end_common_index = end_common_index\n",
    "#             print(list_sentence_token)\n",
    "#             print(greatest_common_length)\n",
    "#             print(greatest_end_common_index)\n",
    "#             print(\"longest_common_length: {0}\".format(longest_common_length))\n",
    "#             print(\"end_common_index: {0}\".format(end_common_index))\n",
    "#             print(\"1. greatest_common_length : {0}\".format(greatest_common_length))\n",
    "            # print(max_match_entity)\n",
    "            # print(\"2. greatest entity : {0}\".format(list_entity[greatest_entity_index]))\n",
    "#             print(\"2.1 greatest_end_common_index: {0}\".format(greatest_end_common_index))\n",
    "#             print(\"3. sentence match: {0}\".format(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1]))\n",
    "            if greatest_common_length != None:\n",
    "                if greatest_common_length >= map_entity_name_to_threshold[entity_name] and max_match_entity > matching_threshold:\n",
    "                    # if entity_name in ['name_activity','type_activity']:\n",
    "                    #     result = list_entity[greatest_entity_index]\n",
    "                    # else:\n",
    "                    #     result = ' '.join(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1])\n",
    "                    \n",
    "                    result = ' '.join(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1])\n",
    "                    if entity_name in result_entity_dict:\n",
    "    #                     result_entity_dict[entity_name].append(list_entity[greatest_entity_index])\n",
    "                        result_entity_dict[entity_name].append(result)\n",
    "                    else:\n",
    "    #                     result_entity_dict[entity_name] = [list_entity[greatest_entity_index]]\n",
    "                        result_entity_dict[entity_name] = [result]\n",
    "    #                 list_sentence_token = list_sentence_token[:greatest_end_common_index - greatest_common_length + 1] + list_sentence_token[greatest_end_common_index +1 :]\n",
    "                    list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1] = [\"✪\"]*greatest_common_length\n",
    "                    normalized_input_sentence = ' '.join(list_sentence_token)\n",
    "            catch_entity_threshold_loop = catch_entity_threshold_loop + 1\n",
    "            # print(\"output sentence: {0}\".format(normalized_input_sentence))\n",
    "    return result_entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✪ ✪ ✪ diễn ra hiến máu tình nguyện đợt ii ở hội trường lớn đh ngân hàng vậy\n",
      "{'intent': 'request', 'inform_slots': {'time': ['hội trường'], 'name_activity': ['hiến máu tình nguyện đợt ii'], 'holder': ['đh ngân hàng']}, 'request_slots': {'time': 'UNK'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "CONSTANT_FILE_PATH = 'constants.json'\n",
    "with open(CONSTANT_FILE_PATH) as f:\n",
    "    constants = json.load(f)\n",
    "\n",
    "file_path_dict = constants['db_file_paths']\n",
    "DATABASE_FILE_PATH = file_path_dict['database']\n",
    "\n",
    "database= json.load(open(DATABASE_FILE_PATH,encoding='utf-8'))\n",
    "state_tracker = StateTracker(database, constants)\n",
    "dqn_agent = DQNAgent(state_tracker.get_state_size(), constants)\n",
    "#TEST\n",
    "if __name__ == '__main__':\n",
    "    print(process_message_to_user_request(\"khi nào thì diễn ra hiến máu tình nguyện đợt ii ở hội trường lớn đh ngân hàng vậy\",state_tracker))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('list_constants.json','r') as constant_file:\n",
    "#     constant = json.load(constant_file)\n",
    "#     list_pattern_time = constant['list_pattern_time']\n",
    "#     res = []\n",
    "#     for pattern in list_pattern_time:\n",
    "#         if \"chủ nhật\" not in pattern:\n",
    "#             res.append(pattern)\n",
    "            \n",
    "#     constant['list_pattern_time'] = res\n",
    "#     with open('list_constants_new.json', 'w+') as new_constant_file:\n",
    "#         json.dump(constant,new_constant_file,ensure_ascii=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_data/real_db_494.json','r') as db_file:\n",
    "    list_data = json.load(db_file)\n",
    "    for data in list_data:\n",
    "        for key, list_value in data.items():\n",
    "            if key != '_id':\n",
    "                list_value = [compound2unicode(value).lower() for value in list_value]\n",
    "                data[key] = list(set(list_value))\n",
    "    with open('temp_data/real_db_494_final.json','w+') as new_db_file:\n",
    "        json.dump(list_data,new_db_file,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DuongCao",
   "language": "python",
   "name": "duongcao2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
