#!/usr/bin/env python
# coding: utf-8
# from agent_utils.state_tracker import StateTracker
from sklearn.externals import joblib
import numpy as np
from sklearn import preprocessing
from sklearn.feature_extraction.text import TfidfVectorizer
from fastai.text import *
import torch
import pickle
import numpy as np
from sklearn.externals import joblib
from sklearn import preprocessing
from constants import *
import json
from collections import OrderedDict
import re



def sentence_to_index_vector(input_sentence):
  list_token=input_sentence.split(' ')
  return vocab.numericalize(list_token)



def forward_dropout(input_sentence):
  t = torch.tensor([sentence_to_index_vector(input_sentence)])
  lm.reset()
  raw_output, dropout_output = lm[0](t)
  
  dropout_output_last_lst=dropout_output[2].detach().numpy().tolist()
  dropout_output_last_lst=dropout_output_last_lst[0]
#   #print(dropout_output_last_lst)
  max_pooling_lst=[]
  avg_pooling_lst=[]
#   print(len(dropout_output_last_lst))
  for i in range(emb_sz):
    lst_one_emb=[]
    for j in range(len(dropout_output_last_lst)-1):
        lst_one_emb.append(dropout_output_last_lst[j][i])
    if len(dropout_output_last_lst)==1:
        max_pooling_lst.append(dropout_output_last_lst[0][i])
        avg_pooling_lst.append(dropout_output_last_lst[0][i])
    else:    
        max_pooling_lst.append(max(lst_one_emb))
        avg_pooling_lst.append(sum(lst_one_emb) / len(lst_one_emb) )
  return max_pooling_lst+avg_pooling_lst+dropout_output_last_lst[-1]


#message -> intent want information
def extract_and_get_intent(message):
        # 10 intent pattern matching

    for notification in list_address_notification:
        if message.lower().find(notification)!=-1:
            return 'address',1.0,message

    for notification in list_name_place_notification:
        if message.lower().find(notification)!=-1 and "liên lạc" not in message.lower() and "liên hệ" not in message.lower() and "đăng kí" not in message.lower() and "đăng ký" not in message.lower():
            return 'name_place',1.0,message


    for notification in list_type_activity_notification:
        if message.lower().find(notification)!=-1:
            return 'type_activity',1.0,message

    for notification in list_name_activity_notification:
        if message.lower().find(notification)!=-1:
            return 'name_activity',1.0,message

    

    for notification in list_time_notification:
        if message.lower().find(notification)!=-1 and "đăng ký" not in message.lower() and "đăng kí" not in message.lower() and "có khi nào" not in message.lower():
            return 'time',1.0,message

    for notification in list_holder_notification:
        if message.lower().find(notification)!=-1 and "liên lạc" not in message.lower() and "liên hệ" not in message.lower() and "đăng kí" not in message.lower() and "đăng ký" not in message.lower() and "sdt" not in message.lower() and "số điện thoại" not in message.lower() and "email" not in message.lower() and "sđt" not in message.lower() and "facebook" not in message.lower() and "fb" not in message.lower():
            return 'holder',1.0,message

    for notification in list_reward_notification:
        if message.lower().find(notification)!=-1:
            return 'reward',1.0,message
        
    
    # 5 intent machine learning 
    

    max_proba=np.amax(clf.predict_proba([forward_dropout(message)])[0])

    #print(clf.predict_proba([forward_dropout(message)])[0])
    #print(max_proba)
    if max_proba>CONST_THRESHOLD:
        if le.inverse_transform(clf.predict([forward_dropout(message.lower())]))[0] == "work":
            return "works",max_proba,message
        return le.inverse_transform(clf.predict([forward_dropout(message.lower())]))[0],max_proba,message


    return 'other',1.0,message


#message -> check intent want information 
def check_intent(message):
#     ... subject muốn hỏi/biết/xin… *
# ... subject muốn được hỏi/tư vấn .... *
# ... cho subject hỏi/biết/xin *
# ... subject cần hỏi/biết/xin... *
# .... subject cần ... thông tin … *
# .... gửi subject ... *
# ... cho hỏi/xin/biết… *
# ... có/được (...)? không (...) *
    #bắt WH question 
    for signal in list_question_signal:
        if signal in message.lower():
            # #print(signal)
            return True

    for verb in list_verb_have:
        if (message.lower().find(verb)!=-1 and message.lower().find("không")!=-1 and message.lower().find(verb)<message.lower().find("không")):
            #print("1")
            return True

    #....sao (liên hệ/đăng ký sao)
    if "sao"==message.split(' ')[len(message.split(' '))-1]:
        return True


    #.... sao bạn
    for object in list_object:
        if message.lower().find("sao")!=-1 and message.lower().find(object)!=-1 and message.lower().find("sao")<message.lower().find(object):
            return True

    if (message.lower().find("còn")!=-1 and message.lower().find("không")!=-1 and message.lower().find("còn")<message.lower().find("không")):
        #print("1")
        return True
    
    if message.lower().find("xin")!=-1 and (message.lower().find("chào")< message.lower().find("xin")):
        #print("1")
        return True

    #cách liên hệ/đăng ký
    if message.lower().find("cách")==0:
        #print("1")
        return True

    #ai .... (ai được tham gia)
    if message.lower().find("ai")==0:
        #print("1")
        return True


    # #thông tin về xxx/thông tin xxx
    # if message.lower().find("thông tin")!=-1:
    #     #print("1")
    #     return True

    for subject in list_subject:
        for verb in list_verb_want:
            if (message.lower().find(subject+" muốn "+verb)!=-1 or message.lower().find("cho "+subject+" "+verb)!=-1 or message.lower().find(subject+" cần "+verb)!=-1):
                #print("2")
                return True

    for subject in list_subject:
        #chứ bạn
        if message.lower().find("chứ "+subject)!=-1:
            #print("3")
            return True
        if (message.lower().find(subject+" muốn được hỏi")!=-1 or message.lower().find(subject+" muốn được tư vấn")!=-1):
            #print("3")
            return True
        if (message.lower().find(subject+" cần")!=-1 and message.lower().find("thông tin")!=-1 and message.lower().find(subject+" cần")<message.lower().find("thông tin")):
            #print("4")
            return True
        if (message.lower().find(subject+" muốn")!=-1 and message.lower().find("thông tin")!=-1 and message.lower().find(subject+" muốn")<message.lower().find("thông tin")):
            #print("4")
            return True
        if (message.lower().find("gửi "+subject)!=-1):
            #print("5")
            return True
        if (message.lower().find("chỉ "+subject)!=-1):
            #print("5")
            return True
        if (message.lower().find("chỉ giúp "+subject)!=-1):
            #print("5")
            return True

    #cho xin
    for verb in list_verb_want:
        if (message.lower().find("cho "+verb)!=-1):
            #print("6")
            return True

    # nào ... nhỉ
    for signal in list_question_signal_last:
        if (message.lower().find("nào")!=-1) and (message.lower().find(signal)!=-1) and (message.lower().find("nào")<message.lower().find(signal)):
            #print("6")
            return True

    #...... hả
    if message.lower().split(' ')[len(message.lower().split(' '))-1]=="hả":
        return True
    

    
    #gửi cho mình/cho mình/gửi mình/mình cần/mình muốn
    for subject in list_subject:
        if (message.lower().find("cho "+subject)!=-1) or (message.lower().find("gửi "+subject)!=-1) or (message.lower().find(subject+" cần")!=-1) or (message.lower().find(subject+" muốn")!=-1):
            return True


    #mình (có việc) muốn/cần
    #mình định....
    for subject in list_subject:
        if ((message.lower().find(subject)!=-1) and (message.lower().find("định")!=-1) and (message.lower().find(subject)<message.lower().find("định"))) or ((message.lower().find(subject)!=-1) and (message.lower().find("cần")!=-1) and (message.lower().find(subject)<message.lower().find("cần"))) or ((message.lower().find(subject)!=-1) and (message.lower().find("muốn")!=-1) and (message.lower().find(subject)<message.lower().find("muốn"))):
            return True

    #bắt YES-NO/WH question mà signal cuối câu 
    if len(message.split(" "))>3 and (message.split(" ")[-1].lower()=="chưa" or message.split(" ")[-1].lower()=="không" or message.split(" ")[-1].lower()=="ta" or message.split(" ")[-1].lower()=="sao" or message.split(" ")[-1].lower()=="nhỉ" or message.split(" ")[-1].lower()=="nào"):
        #print("7")
        return True

    #bắt YES-NO question cuối câu có chủ ngữ

    for subject in list_object:
        for question_signal_last in list_question_signal_last:
            if message.split(" ")[-1].lower()==subject and message.split(" ")[-2].lower()==question_signal_last:
                #print("8")
                return True 
    

    return False



#message -> final output 
def catch_intent(message):
    message_preprocessed = re.sub('[\:\_=\+\-\#\@\$\%\$\\(\)\~\@\;\'\|\<\>\]\[\"\–“”…*]',' ',message)
    message_preprocessed=message_preprocessed.replace(',', ' , ')
    message_preprocessed=message_preprocessed.replace('.', ' . ')
    message_preprocessed=message_preprocessed.replace('!', ' ! ')
    message_preprocessed=message_preprocessed.replace('&', ' & ')
    message_preprocessed=message_preprocessed.replace('?', ' ? ')
    message_preprocessed = compound2unicode(message_preprocessed)
    list_token=message_preprocessed.split(' ')
    while '' in list_token:
        list_token.remove('')
    message_preprocessed=' '.join(list_token)
    #note : vì notification của anything dễ bị nhầm với check intent (do chứa "cái gì","sao")
    # nên check trước


    for notification in list_anything_notification:   
        if message_preprocessed.lower().find(notification)!=-1:
            return 'anything',1.0,message_preprocessed

    if check_intent(message_preprocessed):
            #remove ? with blank in the last
        message_preprocessed=re.sub('[?]','',message_preprocessed.lower())
        list_token=message_preprocessed.split(' ')
        while '' in list_token:
            list_token.remove('')
        message_preprocessed=' '.join(list_token)
        return extract_and_get_intent(message_preprocessed)

    message_preprocessed=re.sub('[?]','',message_preprocessed.lower())
    list_token=message_preprocessed.split(' ')
    while '' in list_token:
        list_token.remove('')
    message_preprocessed=' '.join(list_token)

    for notification in list_done_notification:   
        if message_preprocessed.lower().find(notification)!=-1:
            return 'done',1.0,message_preprocessed

    for notification in list_hello_notification:   
        if message_preprocessed.lower().find(notification)!=-1:
            return 'hello',1.0,message_preprocessed
    
    for notification in list_thanks_notification:   
        if message_preprocessed.lower().find(notification)!=-1:
            return 'thanks',1.0,message_preprocessed



    for object in list_object:
        if message_preprocessed.lower().find("hi " +object)!=-1 and len(message_preprocessed.split(' '))<=5:
            return 'hello',1.0,message_preprocessed
    if message_preprocessed.lower()=="hi":
        return "hello",1.0,message_preprocessed

    return 'not intent',1.0,message_preprocessed

def compound2unicode(text):
  #https://gist.github.com/redphx/9320735`
  text = text.replace("\u0065\u0309", "\u1EBB")    # ẻ
  text = text.replace("\u0065\u0301", "\u00E9")    # é
  text = text.replace("\u0065\u0300", "\u00E8")    # è
  text = text.replace("\u0065\u0323", "\u1EB9")    # ẹ
  text = text.replace("\u0065\u0303", "\u1EBD")    # ẽ
  text = text.replace("\u00EA\u0309", "\u1EC3")    # ể
  text = text.replace("\u00EA\u0301", "\u1EBF")    # ế
  text = text.replace("\u00EA\u0300", "\u1EC1")    # ề
  text = text.replace("\u00EA\u0323", "\u1EC7")    # ệ
  text = text.replace("\u00EA\u0303", "\u1EC5")    # ễ
  text = text.replace("\u0079\u0309", "\u1EF7")    # ỷ
  text = text.replace("\u0079\u0301", "\u00FD")    # ý
  text = text.replace("\u0079\u0300", "\u1EF3")    # ỳ
  text = text.replace("\u0079\u0323", "\u1EF5")    # ỵ
  text = text.replace("\u0079\u0303", "\u1EF9")    # ỹ
  text = text.replace("\u0075\u0309", "\u1EE7")    # ủ
  text = text.replace("\u0075\u0301", "\u00FA")    # ú
  text = text.replace("\u0075\u0300", "\u00F9")    # ù
  text = text.replace("\u0075\u0323", "\u1EE5")    # ụ
  text = text.replace("\u0075\u0303", "\u0169")    # ũ
  text = text.replace("\u01B0\u0309", "\u1EED")    # ử
  text = text.replace("\u01B0\u0301", "\u1EE9")    # ứ
  text = text.replace("\u01B0\u0300", "\u1EEB")    # ừ
  text = text.replace("\u01B0\u0323", "\u1EF1")    # ự
  text = text.replace("\u01B0\u0303", "\u1EEF")    # ữ
  text = text.replace("\u0069\u0309", "\u1EC9")    # ỉ
  text = text.replace("\u0069\u0301", "\u00ED")    # í
  text = text.replace("\u0069\u0300", "\u00EC")    # ì
  text = text.replace("\u0069\u0323", "\u1ECB")    # ị
  text = text.replace("\u0069\u0303", "\u0129")    # ĩ
  text = text.replace("\u006F\u0309", "\u1ECF")    # ỏ
  text = text.replace("\u006F\u0301", "\u00F3")    # ó
  text = text.replace("\u006F\u0300", "\u00F2")    # ò
  text = text.replace("\u006F\u0323", "\u1ECD")    # ọ
  text = text.replace("\u006F\u0303", "\u00F5")    # õ
  text = text.replace("\u01A1\u0309", "\u1EDF")    # ở
  text = text.replace("\u01A1\u0301", "\u1EDB")    # ớ
  text = text.replace("\u01A1\u0300", "\u1EDD")    # ờ
  text = text.replace("\u01A1\u0323", "\u1EE3")    # ợ
  text = text.replace("\u01A1\u0303", "\u1EE1")    # ỡ
  text = text.replace("\u00F4\u0309", "\u1ED5")    # ổ
  text = text.replace("\u00F4\u0301", "\u1ED1")    # ố
  text = text.replace("\u00F4\u0300", "\u1ED3")    # ồ
  text = text.replace("\u00F4\u0323", "\u1ED9")    # ộ
  text = text.replace("\u00F4\u0303", "\u1ED7")    # ỗ
  text = text.replace("\u0061\u0309", "\u1EA3")    # ả
  text = text.replace("\u0061\u0301", "\u00E1")    # á
  text = text.replace("\u0061\u0300", "\u00E0")    # à
  text = text.replace("\u0061\u0323", "\u1EA1")    # ạ
  text = text.replace("\u0061\u0303", "\u00E3")    # ã
  text = text.replace("\u0103\u0309", "\u1EB3")    # ẳ
  text = text.replace("\u0103\u0301", "\u1EAF")    # ắ
  text = text.replace("\u0103\u0300", "\u1EB1")    # ằ
  text = text.replace("\u0103\u0323", "\u1EB7")    # ặ
  text = text.replace("\u0103\u0303", "\u1EB5")    # ẵ
  text = text.replace("\u00E2\u0309", "\u1EA9")    # ẩ
  text = text.replace("\u00E2\u0301", "\u1EA5")    # ấ
  text = text.replace("\u00E2\u0300", "\u1EA7")    # ầ
  text = text.replace("\u00E2\u0323", "\u1EAD")    # ậ
  text = text.replace("\u00E2\u0303", "\u1EAB")    # ẫ
  text = text.replace("\u0045\u0309", "\u1EBA")    # Ẻ
  text = text.replace("\u0045\u0301", "\u00C9")    # É
  text = text.replace("\u0045\u0300", "\u00C8")    # È
  text = text.replace("\u0045\u0323", "\u1EB8")    # Ẹ
  text = text.replace("\u0045\u0303", "\u1EBC")    # Ẽ
  text = text.replace("\u00CA\u0309", "\u1EC2")    # Ể
  text = text.replace("\u00CA\u0301", "\u1EBE")    # Ế
  text = text.replace("\u00CA\u0300", "\u1EC0")    # Ề
  text = text.replace("\u00CA\u0323", "\u1EC6")    # Ệ
  text = text.replace("\u00CA\u0303", "\u1EC4")    # Ễ
  text = text.replace("\u0059\u0309", "\u1EF6")    # Ỷ
  text = text.replace("\u0059\u0301", "\u00DD")    # Ý
  text = text.replace("\u0059\u0300", "\u1EF2")    # Ỳ
  text = text.replace("\u0059\u0323", "\u1EF4")    # Ỵ
  text = text.replace("\u0059\u0303", "\u1EF8")    # Ỹ
  text = text.replace("\u0055\u0309", "\u1EE6")    # Ủ
  text = text.replace("\u0055\u0301", "\u00DA")    # Ú
  text = text.replace("\u0055\u0300", "\u00D9")    # Ù
  text = text.replace("\u0055\u0323", "\u1EE4")    # Ụ
  text = text.replace("\u0055\u0303", "\u0168")    # Ũ
  text = text.replace("\u01AF\u0309", "\u1EEC")    # Ử
  text = text.replace("\u01AF\u0301", "\u1EE8")    # Ứ
  text = text.replace("\u01AF\u0300", "\u1EEA")    # Ừ
  text = text.replace("\u01AF\u0323", "\u1EF0")    # Ự
  text = text.replace("\u01AF\u0303", "\u1EEE")    # Ữ
  text = text.replace("\u0049\u0309", "\u1EC8")    # Ỉ
  text = text.replace("\u0049\u0301", "\u00CD")    # Í
  text = text.replace("\u0049\u0300", "\u00CC")    # Ì
  text = text.replace("\u0049\u0323", "\u1ECA")    # Ị
  text = text.replace("\u0049\u0303", "\u0128")    # Ĩ
  text = text.replace("\u004F\u0309", "\u1ECE")    # Ỏ
  text = text.replace("\u004F\u0301", "\u00D3")    # Ó
  text = text.replace("\u004F\u0300", "\u00D2")    # Ò
  text = text.replace("\u004F\u0323", "\u1ECC")    # Ọ
  text = text.replace("\u004F\u0303", "\u00D5")    # Õ
  text = text.replace("\u01A0\u0309", "\u1EDE")    # Ở
  text = text.replace("\u01A0\u0301", "\u1EDA")    # Ớ
  text = text.replace("\u01A0\u0300", "\u1EDC")    # Ờ
  text = text.replace("\u01A0\u0323", "\u1EE2")    # Ợ
  text = text.replace("\u01A0\u0303", "\u1EE0")    # Ỡ
  text = text.replace("\u00D4\u0309", "\u1ED4")    # Ổ
  text = text.replace("\u00D4\u0301", "\u1ED0")    # Ố
  text = text.replace("\u00D4\u0300", "\u1ED2")    # Ồ
  text = text.replace("\u00D4\u0323", "\u1ED8")    # Ộ
  text = text.replace("\u00D4\u0303", "\u1ED6")    # Ỗ
  text = text.replace("\u0041\u0309", "\u1EA2")    # Ả
  text = text.replace("\u0041\u0301", "\u00C1")    # Á
  text = text.replace("\u0041\u0300", "\u00C0")    # À
  text = text.replace("\u0041\u0323", "\u1EA0")    # Ạ
  text = text.replace("\u0041\u0303", "\u00C3")    # Ã
  text = text.replace("\u0102\u0309", "\u1EB2")    # Ẳ
  text = text.replace("\u0102\u0301", "\u1EAE")    # Ắ
  text = text.replace("\u0102\u0300", "\u1EB0")    # Ằ
  text = text.replace("\u0102\u0323", "\u1EB6")    # Ặ
  text = text.replace("\u0102\u0303", "\u1EB4")    # Ẵ
  text = text.replace("\u00C2\u0309", "\u1EA8")    # Ẩ
  text = text.replace("\u00C2\u0301", "\u1EA4")    # Ấ
  text = text.replace("\u00C2\u0300", "\u1EA6")    # Ầ
  text = text.replace("\u00C2\u0323", "\u1EAC")    # Ậ
  text = text.replace("\u00C2\u0303", "\u1EAA")    # Ẫ
  return text


def longest_common_sublist(a, b):
    table = {}
    l = 0
    i_max = None
    j_max = None
    for i, ca in enumerate(a, 1):
        for j, cb in enumerate(b, 1):
            if ca == cb:
                table[i, j] = table.get((i - 1, j - 1), 0) + 1
                if table[i, j] > l:
                    l = table[i, j]
                    i_max=i
                    j_max=j
    if i_max != None:
        return l,i_max - 1
    return l,i_max

def lcs_length_ta(x, y):
    m = len(x)
    n = len(y)
    c = [[0 for x in range(n+1)] for y in range(m+1)] 
    b = [['' for x in range(n+1)] for y in range(m+1)] 
    
    for i in range(1,m+1):
        for j in range(1,n+1):
            if x[i-1] == y[j-1]:
                c[i][j] = c[i-1][j-1] + 1
                b[i][j] = '↖️'
            elif c[i - 1][j] >= c[i][j-1]:
                c[i][j] = c[i - 1][j] 
                b[i][j] = '⬆️'
            else:
                c[i][j] = c[i][j-1] 
                b[i][j] = '⬅️'
    max_common_length = c[-1][-1]
    i = m
    j = n
    pointer = c[i][j]
    result_index = []
    i_max = 0
    max_length_in_sentence = 0
    while pointer != 0:
        if pointer == max_common_length and b[i][j]=='⬆️':
            i = i-1
            pointer = c[i][j]
        if pointer == max_common_length and b[i][j]=='⬅️':
            j = j-1
            pointer = c[i][j]
        if pointer == max_common_length and b[i][j]=='↖️':
            result_index.append(i)
            i = i-1
            j = j-1
            pointer = c[i][j]
        if pointer != max_common_length and b[i][j]=='⬆️':
            result_index.append(i)
            i = i-1
            pointer = c[i][j]
        if pointer != max_common_length and b[i][j]=='⬅️':
            result_index.append(i)
            j = j-1
            pointer = c[i][j]
        if pointer != max_common_length and b[i][j]=='↖️':
            result_index.append(i)
            j = j-1
            i = i-1
            pointer = c[i][j]
#     print(result_index)
#     print(max_common_length)
    if result_index != [] :
        i_max = result_index[0]-1
        max_length_in_sentence = result_index[0] -result_index[-1] +1
    
    return max_common_length,max_length_in_sentence,i_max
    
def find_entity_longest_common(sentence,list_entity,entity_name):
    normalized_sentence=compound2unicode(sentence)
    list_token_sentence = normalized_sentence.split(' ')
    list_result_entity = []
    dict_max_len = {}
    list_normalized_entity = [compound2unicode(entity) for entity in list_entity]
    result = []
    longest_common_length = None
    end_common_index = None
    for index, entity in enumerate(list_normalized_entity):
#         print(entity)
        list_token_entity = entity.split(' ')
        if entity_name in ['register','reward','works','joiner']:
            longest_common_length, max_length_in_sentence, end_common_index = lcs_length_ta(list_token_sentence,list_token_entity)
        else:
            longest_common_length, end_common_index = longest_common_sublist(list_token_sentence,list_token_entity)
#         print(longest_common_length)
        if longest_common_length!=0:
            if entity_name in ['register','reward','works']:
                longest_common_length_entity, max_length_in_sentence_entity, end_common_index_entity = lcs_length_ta(list_token_entity,list_token_sentence)
                if float(longest_common_length)/max_length_in_sentence > 0.6 and float(longest_common_length_entity)/max_length_in_sentence_entity>0.6:
                    dict_max_len[str(index)] = {'max_length_in_sentence':max_length_in_sentence,'longest_common_length':longest_common_length,'end_common_index':end_common_index}
            else:
                dict_max_len[str(index)] = {'longest_common_length':longest_common_length,'end_common_index':end_common_index}
#         list_token_sentence = list_token_sentence[: end_common_index - longest_common_length] + list_token_sentence[end_common_index:]
    max_longest_common_length=0
#     print(dict_max_len)
    for k,v in dict_max_len.items():
        if v['longest_common_length']>max_longest_common_length:
            max_longest_common_length=v['longest_common_length']
    
    for k,v in dict_max_len.items():
        if v['longest_common_length']==max_longest_common_length:
            if entity_name in ['register','reward','works']:
                result.append({"longest_common_entity_index":int(k),"longest_common_length":v['max_length_in_sentence'],"end_common_index":v['end_common_index']})
            else:
                result.append({"longest_common_entity_index":int(k),"longest_common_length":v['longest_common_length'],"end_common_index":v['end_common_index']})
    return result

def delete_extra_word(sentence,list_extra_word):
    for word in list_extra_word:
        if word == " hả ":
            sentence = sentence.replace(word," ✪ ")
        elif word == " ai ":
            sentence = sentence.replace(word," ✪ ")
        elif word == " ai":
            sentence = sentence.replace(word," ✪")
        elif word in list_question_signal_last and word == sentence.split(' ')[len(sentence.split(' '))-1]:
            sentence = sentence.replace(word,"✪")
        else:# replace la sao -> ✪ ✪
            sentence = sentence.replace(word,' '.join(['✪']*(word.count(' ')+1)))
    return sentence

def hasNumbers(inputString):
    return any(char.isdigit() for char in inputString)

def replace_digit_in_string(inputString):
    return ''.join(['\d' if char.isdigit() else char for char in inputString])

def delete_last_space_list(list_input_string):
    result = []
    for input_string in list_input_string:
        if input_string[len(input_string)-1]==' ':
            result.append(input_string[:len(input_string)-1])
            continue
        result.append(input_string)
    return result

def find_all_entity(intent,input_sentence):
    print("duongcc")
    normalized_input_sentence = compound2unicode(input_sentence)
    normalized_input_sentence = delete_extra_word(normalized_input_sentence,list_extra_word)
    
    result_entity_dict={}
    list_order_entity_name=map_intent_to_list_order_entity_name[intent]
    print(normalized_input_sentence)
    if 'time' in list_order_entity_name:
        for pattern_time in list_pattern_time:
            if re.findall(pattern_time,normalized_input_sentence)!=[]:
                # print("pattern_time :{0}".format(pattern_time))
                if 'time' not in result_entity_dict:
                    result_entity_dict['time'] = delete_last_space_list(re.findall(pattern_time,normalized_input_sentence))
                else:
                    result_entity_dict['time'].extend(delete_last_space_list(re.findall(pattern_time,normalized_input_sentence)))
                
                normalized_input_sentence = re.sub(pattern_time,' '.join(['✪']*(pattern_time.count(' ')+1)),normalized_input_sentence)
        # if 'time' in result_entity_dict:
        #     print(result_entity_dict['time'])
    if 'reward' in list_order_entity_name:
        for pattern_reward in list_pattern_reward:
            if re.findall(pattern_reward,normalized_input_sentence)!=[]:
                print("pattern_reward :{0}".format(pattern_reward))
                if 'reward' not in result_entity_dict:
                    result_entity_dict['reward'] = delete_last_space_list(re.findall(pattern_reward,normalized_input_sentence))
                else:
                    result_entity_dict['reward'].extend(delete_last_space_list(re.findall(pattern_reward,normalized_input_sentence)))
                
                normalized_input_sentence = re.sub(pattern_reward,' '.join(['✪']*(pattern_reward.count(' ')+1)),normalized_input_sentence)
        # if 'reward' in result_entity_dict:
        #     print(result_entity_dict['reward'])
    matching_threshold = 0.0
    longest_common_length, end_common_index = None, None
    
    map_entity_name_to_threshold={}
    for entity_name in list_order_entity_name:
        if entity_name in ['time','address']:
            map_entity_name_to_threshold[entity_name]=1
        elif entity_name in ['name_activity','contact','joiner','holder','type_activity','name_place']:
            map_entity_name_to_threshold[entity_name]=2
        elif entity_name in ['works','reward']:
            map_entity_name_to_threshold[entity_name]=2
        elif entity_name in ['register']:
            map_entity_name_to_threshold[entity_name]=3


    ordered_real_dict = OrderedDict()
    for entity_name in map_intent_to_list_order_entity_name[intent]:
        ordered_real_dict[entity_name] = real_dict[entity_name]
    for entity_name, list_entity in ordered_real_dict.items():
        # print(entity_name)
        list_entity = [entity.lower() for entity in list_entity]
        # print("input sentence: {0}".format(normalized_input_sentence))
        if entity_name in ["works","register","reward"]:
            matching_threshold = 0.15
        elif entity_name == "joiner":
            matching_threshold = 0.2
        else:
            matching_threshold = 0.5
        print("000. sentence:{0}".format(normalized_input_sentence))
        catch_entity_threshold_loop = 0
        while True:
            if catch_entity_threshold_loop > 5:
                break
            list_dict_longest_common_entity = find_entity_longest_common(normalized_input_sentence,list_entity,entity_name)
#             print(list_dict_longest_common_entity)
                #     [{'longest_common_entity_index': 0,
                #   'longest_common_length': 3,
                #   'end_common_index': 9}]
            

            ##find the most match longest common match (calculate by length of token match in sentence 
                                                                #/ length of entity )
                            ##{'greatest_match_entity_index':0,'longest_common_length':3,'end_common_index':9}
            if list_dict_longest_common_entity == []:
                break
            if list_dict_longest_common_entity[0]['longest_common_length'] < map_entity_name_to_threshold[entity_name] :
                break
            
            list_sentence_token = normalized_input_sentence.split(' ')
#             print("list_sentence_token :{0}".format(list_sentence_token))
            greatest_entity_index=None
            greatest_common_length = None
            greatest_end_common_index = None
            max_match_entity = 0.0
#             print("common entity :{0}".format(list_dict_longest_common_entity))
            for dict_longest_common_entity in list_dict_longest_common_entity:
#                 print("0. dict_longest_common_entity: {0}".format(dict_longest_common_entity))

#                     print("duong")
#                 print("0.1 entity: {0}".format(list_entity[dict_longest_common_entity['longest_common_entity_index']]))
                longest_common_entity_index = dict_longest_common_entity['longest_common_entity_index']
                longest_common_length = dict_longest_common_entity['longest_common_length']
                end_common_index = dict_longest_common_entity['end_common_index']
                
                list_sentence_token_match = list_sentence_token[end_common_index - longest_common_length+1:end_common_index+1]
                if entity_name == "type_activity":
                    if "ban chỉ huy" in normalized_input_sentence or "ban tổ chức" in normalized_input_sentence or "bch" in normalized_input_sentence or "btc" in normalized_input_sentence:
                        continue
                    #nếu chỉ là các câu inform 1 entity mà câu đó không phải là câu inform tên 1 hoạt động thì không cần xét
                    if "inform" not in intent or "name_activity" in intent:
                        list_name_activity = ordered_real_dict['name_activity']
                        check_in_name = False
                        for name_activity in list_name_activity:
                            #nếu loại hoạt động nằm trọn trong bất kì 1 tên hoạt động 
                            # thì không lấy
                            if  name_activity.find(' '.join(list_sentence_token_match)) > 0:
                                check_in_name = True
                                break
                        if check_in_name == True:
                            continue
                
                if entity_name == "holder":
                    # nếu holder mà trước đó có từ chỉ nơi chốn : ở, tại => không là holder mà là  
                    # name_place
                    if end_common_index - longest_common_length >= 0:
                        if list_sentence_token[end_common_index - longest_common_length] in ["ở","tại","trước","sau","trong"]:
                            if 'name_place' in result_entity_dict:
            #                     result_entity_dict[entity_name].append(list_entity[greatest_entity_index])
                                result_entity_dict['name_place'].append(' '.join(list_sentence_token_match))
                            else:
            #                     result_entity_dict[entity_name] = [list_entity[greatest_entity_index]]
                                result_entity_dict['name_place'] = [' '.join(list_sentence_token_match)]
                            list_sentence_token[end_common_index - longest_common_length +1 :end_common_index +1] = ["✪"]*longest_common_length
                            normalized_input_sentence = ' '.join(list_sentence_token)
                            continue

#                 print("2. list_sentence_token_match : {0}".format(list_sentence_token_match))
                list_temp_longest_entity_token = list_entity[longest_common_entity_index].split(' ')
#                 print("3. list_temp_longest_entity_token : {0}".format(list_temp_longest_entity_token))
                if entity_name in ["works","register","reward"]:
#                     print("list_temp_longest_entity_token :{0}".format(list_temp_longest_entity_token))
#                     print("list_sentence_token_match :{0}".format(list_sentence_token_match))
                    _,longest_common_length_entity,end_common_index_entity = lcs_length_ta(list_temp_longest_entity_token,list_sentence_token_match)
                    list_entity_token_match = list_temp_longest_entity_token[end_common_index_entity - longest_common_length_entity +1 :end_common_index_entity +1]
                    score = len(list_entity_token_match)/float(len(list_temp_longest_entity_token))
#                     print("list_entity_token_match: {0}".format(list_entity_token_match))
#                     print("list_temp_longest_entity_token:{0}".format(list_temp_longest_entity_token))
#                     print("score :{0}".format(score))
                    
                else:
                    score = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))
                if score > max_match_entity:
#                     max_match_entity = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))
                    max_match_entity = score
                    greatest_entity_index = longest_common_entity_index
                    greatest_common_length = longest_common_length
                    greatest_end_common_index = end_common_index
#             print(list_sentence_token)
#             print(greatest_common_length)
#             print(greatest_end_common_index)
#             print("longest_common_length: {0}".format(longest_common_length))
#             print("end_common_index: {0}".format(end_common_index))
#             print("1. greatest_common_length : {0}".format(greatest_common_length))
            # print(max_match_entity)
            # print("2. greatest entity : {0}".format(list_entity[greatest_entity_index]))
#             print("2.1 greatest_end_common_index: {0}".format(greatest_end_common_index))
#             print("3. sentence match: {0}".format(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1]))
            if greatest_common_length != None:
                if greatest_common_length >= map_entity_name_to_threshold[entity_name] and max_match_entity > matching_threshold:
                    # if entity_name in ['name_activity','type_activity']:
                    #     result = list_entity[greatest_entity_index]
                    # else:
                    #     result = ' '.join(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1])
                    
                    result = ' '.join(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1])
                    if entity_name in result_entity_dict:
    #                     result_entity_dict[entity_name].append(list_entity[greatest_entity_index])
                        result_entity_dict[entity_name].append(result)
                    else:
    #                     result_entity_dict[entity_name] = [list_entity[greatest_entity_index]]
                        result_entity_dict[entity_name] = [result]
    #                 list_sentence_token = list_sentence_token[:greatest_end_common_index - greatest_common_length + 1] + list_sentence_token[greatest_end_common_index +1 :]
                    list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1] = ["✪"]*greatest_common_length
                    normalized_input_sentence = ' '.join(list_sentence_token)
            catch_entity_threshold_loop = catch_entity_threshold_loop + 1
            # print("output sentence: {0}".format(normalized_input_sentence))
    return result_entity_dict

def process_message_to_user_request(message,state_tracker):
    
    if isinstance(message,str):
        intent , proba , processed_message = catch_intent(message)
        user_action = {}
        if intent not in ['hello','done','not intent','thanks','anything',"other"]:
            result_entity_dict = find_all_entity(intent,processed_message)
            # print(result_entity_dict)
            # print(intent)
            user_action['intent'] = 'request'
            user_action['inform_slots'] = result_entity_dict
            user_action['request_slots'] = {intent:'UNK'}
        elif intent == 'not intent':
            #get agent request key for user to inform (not intent)
            last_agent_action = state_tracker.history[-1]
            if len(list(last_agent_action['request_slots'].keys())) > 0:
                agent_request_key = list(last_agent_action['request_slots'].keys())[0]
            result_entity_dict = find_all_entity(agent_request_key+"_inform",processed_message)
            user_action['intent'] = 'inform'
            user_action['inform_slots'] = result_entity_dict
            user_action['request_slots'] = {}
        elif intent == 'anything':
            last_agent_action = state_tracker.history[-1]
            anything_key = list(last_agent_action['request_slots'].keys())[0]
            user_action['intent'] = 'inform'
            user_action['inform_slots'] = {anything_key:'anything'}
            user_action['request_slots'] = {}
        else:
            user_action['intent'] = intent
            user_action['inform_slots'] = {}
            user_action['request_slots'] = {}
    else:
        user_action = message
    # print("-----------------------------user action")
    # print(user_action)
    return user_action

#TEST
if __name__ == '__main__':
    # #TESTCASE FILE
    # testcase_file= open("data/testcase_intent_recognizer.txt","r",encoding='utf-8')
    # output_test_file=open("data/output_intent_recognizer_1043_dictionary_replace_all_placeholder.txt","w+",encoding='utf-8')
    # testcases=testcase_file.readlines()
    # num_success_testcases=0
    # num_testcases=len(testcases)
    # i=0
    # for testcase in testcases:
    #     # #print(testcase)
    #     i+=1
    #     print(i)
    #     success=False
    #     testcase_without_enter=testcase.replace('\n', '').split('|')
    #     message=testcase_without_enter[0]
    #     intent_label=testcase_without_enter[1]
    #     intent_predict,proba=process_message(message)
    #     if intent_label==intent_predict:
    #         success=True
    #         num_success_testcases+=1
    #     output_test_file.write("Message: {0} \t Intent_label: {1} \t Intent_predict: {2} \t Probability: {3} \t Success: {4} \n".format(message,intent_label,intent_predict,proba,success))
    # output_test_file.write("Success rate: {0} %".format(100*float(num_success_testcases)/num_testcases))
    # output_test_file.close()
    # testcase_file.close()
    # print(catch_intent("cái gì cũng được"))
    # print(process_message_to_user_request("cho mình xin thông tin đăng kí mùa hè xanh khoa máy tính"))
    # print(find_all_entity("joiner","mình thích âm nhạc thì đi mùa hè xanh khoa máy tính được không",list_extra_word))
    # print(list_extra_word)
    # print(vocab.stoi["sẻ"])
    # state_tracker = StateTracker(database, constants)
    # process_message_to_user_request("cho mình hỏi đối tượng tham gia của hoạt động an toàn thực phẩm và an ninh lương thực",state_tracker)
    pass